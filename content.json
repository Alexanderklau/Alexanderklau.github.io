{"meta":{"title":"Yemilice","subtitle":null,"description":null,"author":"Yemilice lau","url":"https://yemilice.com"},"pages":[{"title":"","date":"2019-11-06T02:28:37.885Z","updated":"2019-11-06T02:28:37.461Z","comments":true,"path":"baidu_verify_kjucdQVPYU.html","permalink":"https://yemilice.com/baidu_verify_kjucdQVPYU.html","excerpt":"","text":"kjucdQVPYU"},{"title":"","date":"2019-10-28T05:11:59.061Z","updated":"2019-10-28T05:11:59.061Z","comments":true,"path":"google9c12e62c3231610a.html","permalink":"https://yemilice.com/google9c12e62c3231610a.html","excerpt":"","text":"google-site-verification: google9c12e62c3231610a.html"},{"title":"关于我","date":"2020-04-14T11:23:29.000Z","updated":"2020-05-19T08:27:18.126Z","comments":true,"path":"about/index.html","permalink":"https://yemilice.com/about/index.html","excerpt":"","text":"没什么优点，一个写代码的说唱歌手。 其实细细对比一下，写代码和玩说唱的开始的时间都差不多，都开始于2014年 那时候我还是个大二学生，哈哈，找到第一份兼职，那个夏天我不会忘记 蜗居在租来的小房子里，一台二手苹果电脑，风扇带着炎热拍打着我的脸 耳边充斥着室友们的喧闹，睁开眼已经是2020年，所以只是一场大梦而已。 人生如逆旅，我亦是行人。 2016顺利毕业，回到北京，无休止的加班，熬夜，加班，压力伴随着成长，人生在得失之间变幻。 玩音乐的年份可就久啦 小时候拿起的第一把吉它，是被砸碎的音乐梦想 第一次触碰到的钢琴，是母亲的辛苦汗水和谆谆教导 出师未捷身先死的小提琴，是父亲离家远去后的悲伤 哈哈，一切都像是在昨天一样，但是我活在今天，不是吗？ 2016年我写了自己的第一首说唱Demo，正式开始说唱之路。 2018年接触Beats制作，2019年参加地下八英里，依旧还未成功。 2020年，我将写出自己的第一首单曲。 我啊，依旧在不断的向前走 如果你看到了博客，说明你我很幸运能认识，感恩你来过，也感恩你的离开。 2020-04-13"},{"title":"分类","date":"2020-04-14T11:16:58.000Z","updated":"2020-04-14T11:17:49.391Z","comments":true,"path":"categories/index.html","permalink":"https://yemilice.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-05-19T07:34:45.854Z","updated":"2019-10-28T05:11:59.061Z","comments":true,"path":"images/google9c12e62c3231610a.html","permalink":"https://yemilice.com/images/google9c12e62c3231610a.html","excerpt":"","text":"google-site-verification: google9c12e62c3231610a.html"},{"title":"tags","date":"2020-04-14T11:21:09.000Z","updated":"2020-04-14T11:21:09.945Z","comments":true,"path":"tags/index.html","permalink":"https://yemilice.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Golang语言的一些基础(针对面向基础的笔/面试)","slug":"Golang语言的一些基础-针对面向基础的笔-面试","date":"2020-08-25T01:43:13.000Z","updated":"2020-08-25T05:12:44.885Z","comments":true,"path":"2020/08/25/Golang语言的一些基础-针对面向基础的笔-面试/","link":"","permalink":"https://yemilice.com/2020/08/25/Golang%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80-%E9%92%88%E5%AF%B9%E9%9D%A2%E5%90%91%E5%9F%BA%E7%A1%80%E7%9A%84%E7%AC%94-%E9%9D%A2%E8%AF%95/","excerpt":"","text":"前言昨天让无糖信息的面试官嘲讽了之后，我火速写了一篇博客嘲讽回去一波，但是嘲讽归嘲讽，该做的事儿还是要做的，所以昨天晚上花了一些时间总结了一些Golang的基础，作为查漏补缺和自己学习。 还有就是，无糖信息这种公司对待面试者的态度，也决定这个公司的基本格局，反正我是不会去了，如果看到这个blog的人，你们去面无糖信息的Golang还是要用点心，得有个大心脏，因为那个面试官根本就不会care你的感受。 嘿嘿，不过那个面试官，我不知道你的名字，我也不会care你是否能看到，我只希望你以后懂得尊重这个词语的意义。 面试官先生，你应该不会看到我的博客，那我用一些歌词回击你吧，不过看你那个样子，应该不懂音乐（狗头）。 12345自由的灵魂，从来都不需要拟行程，我们的目标在云层。 Golang的理论基础Golang的保留字段有哪些？12345break default func interface selectcase defer go map structchan else goto package switchconst fallthrough if range typecontinue for import return var Golang声明变量的方法？1234567var a int = 1 //第一种: var variable_name variable_namevalue := 1 //第二种: value_name := 1var b, c, d = 1, 2, 3 //第三种: 合并声明var( //第四种: 合并声明 value1 int = 1 value2 string = \"hello world\") Golang声明常量的方法？12345const var a int = 1const var ( b int = 1 c string = \"hello world\") Golang的init函数是什么？程序运行前的注册功能，理解为Python的init也可以 init函数的特性 1234561. init函数先于main函数自动执行，不能被其他函数调用；2. init函数没有输入参数、返回值；3. 每个包可以有多个init函数；4. 包的每个源文件也可以有多个init函数，这点比较特殊；5. 同一个包的init执行顺序，golang没有明确定义，编程时要注意程序不要依赖这个执行顺序。6. 不同包的init函数按照包导入的依赖关系决定执行顺序。 defer是什么？延迟函数的意思 举个例子 下面的函数返回什么值？ 123456func test1() (result int) &#123; defer func() &#123; result++ &#125;() return 0&#125; 此处应该返回一个1 defer后进先出是怎么表现的？举个例子 12345func main() &#123; for i := 0; i &lt; 5; i++ &#123; defer fmt.Printf(\"%d \", i) &#125;&#125; 此处会返回，4，3，2，1，0 Golang的协程是什么？这个问题很宽泛，无糖信息的那个面试官问了我一个很玄幻的问题：“你协程用的多吗？” 我当时很想问，什么叫我协程用的多，Golang有协程，Python也有，用的多不多你看我项目不就完了，分布式，大规模集群的，有几个没用过协程的，这问题就很可笑。 回到主题，协程是什么，按照golang的语法，创建一个协程只需要 1go example() 就可以了。 协程（coroutine）是Go语言中的轻量级线程实现，由Go运行时（runtime）管理。 Golang 的协程本质上其实就是对 IO 事件的封装，并且通过语言级的支持让异步的代码看上去像同步执行的一样。 协程的控制，一般是channel，waitgroup，context之类的，这个我写过文章详细说了，这里就不再描述，如果你们想看，翻一下我以前的blog就可以了。 struct是怎么用的？或者struct是干嘛的？这兄弟名字叫结构体，写过Python的话，你理解为一个固定了格式的dict就可以了 使用的方法 123456789101112131415161718192021import \"fmt\" type Example struct &#123; Name string Age int&#125; func main() &#123; // 声明 e := Example&#123;&#125; fmt.Println(t) t.Name = \"back\" t.Age = 10 fmt.Println(t)&#125; 返回的值:&#123; 0&#125;&#123;back 10&#125; interface是什么？有些东西名义上是接口，其实啥都能干 interface是一种值，它可以像是值一样传递。并且在它的底层，它其实是一个值和类型的元组，interface是一种万能数据类型，它可以接收任何类型的值。 举个例子 123456var a1 interface&#123;&#125; = 1var a2 interface&#123;&#125; = \"abc\"list := make([]interface&#123;&#125;, )list = append(list, a1)list = append(list, a2)fmt.Println(list) 判断interface的值的类型 123456switch v := i.(type) &#123;case int: fmt.Println(\"int\")case string: fmt.Println(\"string\")&#125; interface是一个nil，你理解为Python里面的nil就可以了。 nil也可以调用interfece panic是干嘛的？类似 try-catch-finally 中的 finally，接收一个interface{}类型的值（也就是任何值了）作为参数，Golang没有try catch，所以panic会直接挂掉程序，如果panic中有defer，那么将先会执行defer，然后，再次抛出panic错误，打印堆栈。 多个defer的执行顺序1234func c() (i int) &#123; defer func() &#123; i++ &#125;() return 1&#125; 这时候应该返回 2 一个Go project是如何管理的？很简单，go mod， 创建一个新的工程 1234// 初始化go modgo mod init// 下载依赖包/源码包go mod vendor vendor是一个源码包的集合。这个过于基础了，实在没想到这种题都能考。 slice切片的基础操作把切片理解为Python中的list，这样是不是就简单多了 声明空切片 1var sliceTmp []int 初始化一个切片 1sliceTmp2 := []string&#123;\"a\",\"b\",\"c\"&#125; 修改一个切片的值 1sliceTmp2[0] = \"b\" 追加切片值 1sliceTmp2 = append(sliceTmp2,\"d\") 截取切片段 1var sliceTmp5 = sliceTmp4[1:4] 遍历切片 123for index, value :=range s1&#123; doSomething&#125; make是干嘛的？简单地说，make就是告诉计算机，我要申请内存了，你给我腾地儿。 因为我们对于引用类型的变量，不光要声明它，还要为它分配内容空间 make用于内存分配，但只用于通道chan、映射map以及切片slice的内存创建。 举个例子 123456789101112131415161718192021// 创建一个指定长度的切片mySlice1 := make([]int, 5)//创建一个初始元素长度为5的数组切片，元素初始值为0，并预留10个元素的存储空间： mySlice2 := make([]int, 5, 10) //创建了一个键类型为string、值类型为PersonInfomyMap = make(map[string] PersonInfo) //也可以选择是否在创建时指定该map的初始存储能力，创建了一个初始存储能力为100的map.myMap = make(map[string] PersonInfo, 100) //创建并初始化map的代码.myMap = map[string] PersonInfo&#123; \"1234\": PersonInfo&#123;\"1\", \"Jack\", \"Room 101,...\"&#125;, &#125; //创建有缓存通道ch := make(chan int, 10)//创建无缓存通道ch := make(chan int) Golang中Json忽略字段有些时候有些字段我们要忽略掉，这里要在定义struct的时候做文章，这边采用了定义nil的手段 123456789101112131415type astruct struct &#123; A string B string C interface&#123;&#125;&#125;func main() &#123; var a = astruct&#123; A: \"hello\", b: \"hello\", C: nil &#125;&#125;定义nil直接忽略值就可。 map的一般用法想成Python中的dict，不同的是这东西需要你提前定义 map的常见操作有：声明、赋值、添加、删除、查询、遍历、清空等 123456789101112131415varstuMapmap[int]string //声明 var map名称 map[键类型]值类型mapScore:=make(map[string]float32) //或者这样声明​stuMap=map[int]string&#123;1001:\"Tom\",1002:\"Tim\"&#125; //赋值stuMap[1003] =\"Tem\" //添加delete(stuMap, 1003) //删除​data, flag:=stuMap[1003] //查询该数据是否存在，不存在时flag为false;存在时 //data存储数据，flag为true​forkey, data:=rangestuMap&#123; //遍历键和值 fmt.Println(key, data) delete(stuMap, key) //循环删除清空&#125;stuMap=make(map[int]string) //或者重新make新的空间以清空stuMap，推荐方法 map的常见方法有：键值存在性、排序、嵌套 1234567891011data, flag:=stuMap[1003] //判断存在性，查询该数据是否存在，不存在时flag为false;存在时 //data存储数据，flag为true​import\"sort\" //利用sort包完成排序功能varsortSlice[]int //定义sortSlice切片 forkey, _:=rangestuMap&#123; sortSlice=append(sortSlice, key) //合成切片&#125; sort.Ints(sortSlice)varstuMap2map[int](map[int]string) //嵌套，即值类型可以嵌套其他类型 Golang中的goroutine并发执行有什么规律？首先，并发，不是并行 如果设置了 1runtime.GOMAXPROCS(n) 这个东西相当于告诉程序，此刻你只能有n个协程去并行，那个n相当于就是一个控制因素 1234567891011121314151617181920import ( \"fmt\" \"time\") func ready(w string, sec int64) &#123; time.Sleep(time.Duration(sec * 1e9)) fmt.Println(w, \"is ready!\")&#125;func main() &#123; go ready(\"Tee\", 2) go ready(\"Coffee\", 1) fmt.Println(\"I'm waiting\") time.Sleep(5 * 1e9)&#125; 结果：I'm waitingCoffee is ready!Tee is ready! 结尾细细一看写了不少了，如果能帮到谁，那我真的非常开心，希望大家的技术都能越来越好。 也希望有些面试官真的，认清自己的能力，你的确有地方可以，但是，下场比划比划，你也有不是个数的地方，所以，对技术抱有敬畏之心，对未知抱有好奇之心，对他人抱有尊重之心，才是能走的更远的本质，这个我相信您一定能懂，不过我也不希望你懂，因为就您这态度，我希望你继续保持眼高于顶的态度。因为我希望你35岁被裁员的时候也会记得你有一天这么对待过别人。 嘿嘿，最后用歌词结束这两天不开心的心情吧。 1234567891011121314151617让时间去给看法错的会被正义斩杀你知道天堂很美孩子们千万别喊害怕你想要的东西必须用全力去争取金钱和权力才不是你想要的生活真谛check~","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"似乎要沉下心来处理一些事","slug":"似乎要沉下心来处理一些事","date":"2020-08-24T08:22:14.000Z","updated":"2020-08-24T11:12:54.683Z","comments":true,"path":"2020/08/24/似乎要沉下心来处理一些事/","link":"","permalink":"https://yemilice.com/2020/08/24/%E4%BC%BC%E4%B9%8E%E8%A6%81%E6%B2%89%E4%B8%8B%E5%BF%83%E6%9D%A5%E5%A4%84%E7%90%86%E4%B8%80%E4%BA%9B%E4%BA%8B/","excerpt":"","text":"前言今天面试感觉不是特别好 其实我一直都觉得自己的技术似乎还行，我估摸着”还行”这个定义真的稍微模糊了一点，更多可能就是我什么都懂一些，其实内部什么也不懂 面试的经过真的就算了，毕竟都没到问我核心的那一步。拿了个笔试题让我做，我看了一下基本都是基础，类似校招题那种，返回什么具体值，该怎么返回，xx是干嘛的之类的，这种我就答的很差，错很多，然后华丽GG。 主要是我平常干活基本都是多语言开发混用，有时候还写写自动化测试，写写前端（毕竟全栈。。。），对这种基础我觉得是真的8行，人家面试官也挺给力，判完卷子，直接就说了解了，就这样吧（你可以走了）。 行，那就这样吧，这么多东西你也不问，做了什么你也不问，我做的东西您也不问，算法您也没考，真的不知道怎么说，菜是原罪，我菜我认。 反思具体问题总归是要具体分析，不能像歪嘴赘婿一样，动不动就土味打脸，出一时之气，喊一句“莫欺技术不好，将来让我面试你”就完事儿了，但是讲真，今天还是有点那啥，不甘心 这次面试我发现了我几个很严重的问题 基础技术较差其实很多东西，IDE都帮咱补全了，特别是JB家的IDE，谁用谁知道，设计流程就完事儿了，这次的面试题基本偏向于具体值返回/某个包是干嘛的，其实这种题真的意义不大，真的要考完全可以请出电脑，咱们现场编码，但是我寻思，如果失去IDE，是不是我就是两眼一抹黑？但是IDE存在的意义不就是解放双手？呸，跑题了，回到基础差的主题。 其实有些题我真的知道一个大概，但是就是不知道怎么说，讲真，我觉得我可以和那个面试官好好聊一些别的东西，例如系统设计架构，或者是系统处理，业务高效处理之类的。而不是坐在这里聊一些什么值应该返回什么结果的问题，所以我觉得那个面试官，真的，您似乎不怎么了解我，不过您应该也不care我的想法，同理，我也不care你的。 基础的技术包含了基础的语法，基础的技术思想，或者是一些语法特性，语法糖之类的，如果你是新手，了解这个还是很有必要了，我现在就是复习and查漏补缺，慢慢写手册慢慢复习，对吧。 不知道谁会看到博客，不过我也不在意你们怎么看我，我真的认为啊，像我们这种工作了3年+的人，在每个项目中都扛了很多事儿，更应该了解一下咱们在项目中怎么能更好的处理业务，而不是纠结一个返回值是什么，我个人感觉这次面试完全表现失常，还Tm不如前段时间的几个电话面试发挥的好。您可以认为我是技术无法展现的不甘心，我倒是认为这是面试人员的一种。。。。怎么说呢，我觉得是一种傲慢。 其他我也编不出来了菜，菜，菜，菜菜子的菜，可能就是我太菜，只因为我太菜，哈哈哈。 谈一谈我想要的面试借着自己的这个小天地，我说说我想要什么样子的面试吧。 其实，面试不是考试，评判一个人是否适合公司业务线，或者是是否适合自己的团队，完全可以去引导面试对象，发掘出面试对象的优势。如果答不出来，完全可以走别的逻辑啊，引导到面试官的擅长方向，让面试对象去多说自己的优势，然后再去综合考量是否合适自己的团队。这才是我想要的面试，也是我正在努力的方向。我觉得今天那个面试官就挺xx的，估计一开始也就没认真看，或者就没想诚意招人。 大家都是平等的，真的，您不用高高在上的，觉得自己似乎在一个公司当一个领导是一个很xx的事儿，我也面试过别人，别人来，至少我都是笑着欢迎的，笑着送走的，人家答不出来，至少我还会给出我自己解答然后听他说，我自己是做到这一点的，所以我敢这么说，我面试几个大厂，人家也是这么对我的。 因为这是礼貌问题，也代表我的家庭教养，您这个态度吧，算了算了，对了，你要是看到了，对号入座了，那就很对不起了啊。 我也不知道说什么，就当结尾面试这种东西，本来就是双向的。有些人说，人家可能就是为了完成kpi，或者人家要的不是我这种人，所以你过去不就是自取其辱嘛，不过我不这么想 其实我认为这次倒是给我敲响了一个警钟，第一让我明白了自己的水平是个什么样子，第二让我明白了面试官的水平都是良莠不齐的，你可能可以遇到很好的，或者有点耐心的，你可也能遇到那种话都不说的，无所谓了。 其实说无所谓，心里还是有些不爽，不过这也是自己的一个警示，做事还是要。。以后投简历还是要好好看看，这公司到底需不需要我的技术栈，这次面试全程我和面试官无交流，所以，大家都不要浪费彼此时间了，如果有笔试，请您告诉我，我好好准备，如果没有笔试，咱们好好聊项目，就这样吧。 end，所以我，最近要努力啦！","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://yemilice.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"keywords":[]},{"title":"中文分词的算法分析","slug":"中文分词的算法分析","date":"2020-08-21T01:57:46.000Z","updated":"2020-08-21T03:10:41.908Z","comments":true,"path":"2020/08/21/中文分词的算法分析/","link":"","permalink":"https://yemilice.com/2020/08/21/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/","excerpt":"","text":"前言起因是一次电话面试，面一个技术比较好的公司，我认为自己玩Elasticsearch还是比较久了，还是能交锋几个回合吧，结果人家一问，中文分词的算法，你有了解吗？ 纳尼？中文分词，不就是调一下中文分词器做index嘛，我说了调用IK分词器，人家让我说一下中文分词的算法，或者你有没有了解过，我只有老实的说，我没。 后来当然是有点凉，我觉得我需要去详细看一下这方面的东西。所以这两天除了看房子，我也具体看了一下Elasticsearch和其他的，例如tika，fscrawler之类的分词逻辑，加上网上的一些资料，还是有一些收获，这里做个记录，也是未来的博客文章从基础转向高级的一个转折点吧。 分词是什么？什么是分词？它能干嘛？首先三连，这个我认为比较基础，简单的说，在我们搭建检索引擎或者设计搜索的时候，为了方便能更加简便的搜索出我们想要的东西，首先听一个术语，叫做分词粒度，例如，我们搜索 “我是吴彦祖”，下面是不同的粒度的分析划分。 我，是，吴彦祖 最细粒度 我是，吴彦祖 正常粒度 我，我是，我是吴，吴彦祖，是吴彦祖 混合粒度 你可以看见，不同的分词效果，带来的分词结果不同，举个例子，我们如果根据正常粒度，搜索 “我”，这时就搜索不出来，但是根据混合粒度，我们搜索 “我”，就能得到 “我是吴彦祖” 的搜索结果，所以，良好的分词，是增加搜索效率和搜索结果的重要因素，同理，分词也是一个搜索引擎的老大部分，这次咱们不说屁股的事儿，咱们光说中文，中文的分词算法组成到底有哪些，您往下看。 中文分词的几个算法解析根据我查询的一些资料，和阅读一些技术文档，我总结出了如下几个搜索算法 词典类分词法这个大类其实比较好理解，现在大部分的搜索引擎都是根据中文字典作为分词的方法，它首先是基于中文字典，然后根据算法做分词，具体的算法如下 最大匹配算法（Maximum Matching）这里分为两种，一种是正向最大匹配，一种是逆向最大匹配，这里我主要说一下正向最大匹配方法，下面我都用MM来代替算法名，后面也都一样。 正向最大匹配算法MM正向算法的原理，其实就是将等待分词的文本和字典进行匹配，遵循的是从左往右匹配的原则，如果匹配上了，切分出一个词汇，举个简单的例子 1234# 等待分词的列表wait_seg_word = [\"我\",\"是\",\"吴\",\"彦\",\"祖\"]# 词典word_dicts =&#123;\"我是\", \"吴彦祖\", \"我\", \"是吴彦祖\", \"彦祖\"&#125; 首先从wait_seg_word开始扫描 wait_seg_word[0] = “我” wait_seg_word[1] = “是” 这时候发现了 “我是” 已经在word_dicts中了，可以做切分了，但是我们需要做到最大匹配，所以继续切分 wait_seg_word[2] = “吴”，发觉 “我是吴” 不能够组成词组，继续往下 wait_seg_word[3] = “彦”, 不能组成任何词组 wait_seg_word[4] = “祖”, “是吴彦祖” 匹配了词典，所以最大匹配为 “是吴彦祖” 这就是基础的正向最大匹配算法 逆向最大匹配算法MM逆向算法其实就是MM正向算法的逆袭思维，我们简单说一下吧。 1234# 等待分词的列表wait_seg_word = [\"我是吴彦祖\"]# 词典word_dicts =&#123;\"我是\", \"吴彦祖\", \"我\", \"是吴彦祖\", \"彦祖\"&#125; 定义一个做大分割值为 4，从右往左切分等待分词的列表 取得到 “是吴彦祖”， 发觉它已经在词典中了 去掉最左边第一个字，得到 “吴彦祖” 以此类推，一直到无法被减去为止。 然后再做一次切分，取得到 “我是吴彦” 再次切分，对比。重复上述步骤。 双向最大匹配算法双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。 这个做个比较就好了，我简单说下它是干嘛的吧。 双向切分算法就是使用正向切分一次、逆向切分一次。如果两次切分结果一样的话就好说了，随便选一个结果就可以。 但是如果切分不一样的话使用那一次的切分结果呢？这就涉及到了结果的选取原则问题。切分词应该遵守以下原则： 1：最大匹配原则：上面一直在说这个，使用这个原则的原因是词的字数越多，表示的含义越丰富、对于一条语句分出来的词也就越少，相对的，准确性也就会越高。 2：词库中没有的单字词越少越好。这个原则有点依赖于词库了，至少词库中应该有一些常用的单字成词的字吧，比如：“你”、“我”、“他”、“和”、“的”、“了”等。使用这个原则的原因可以从上面提到的“我是吴彦祖”这个例子看出来： 正向结果：我是、是、吴彦祖 逆向结果：我是、我、是吴彦祖 虽然分出来的结果单字词都是一个，但是，逆向的单字词”和“在词库中存在，所以我们选择返回逆向切分结果。 其实说白了就是很依赖词库，如果没词库就是一个哑巴教一个不会说话的孩子念绕口令，纯属烧脑。。。。 统计类分词法统计分词是什么？其实说白了，通过相邻的字同时出现的次数越多，就越可能构成一个词，也就是出现的频率。同样的词组出现多次，被统计的概率越大，组成合理词组也就越精准，可信。因此字与字相邻出现的概率或频率能较好的反映词的可信度。 根据我查询的资料和实践所得，具体有这么几种算法 N-gram模型算法这个主要运用在tika当中，tika你不会不知道吧？你不知道？后面我写个文章你瞅瞅去吧。。。。 tika是我做全文检索插件的时候接触的，它的核心分词算法就是N-gram算法，这个比较重要，我会详细的说一下这个算法。 首先，N-gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。（这段出自知乎） 一个N-gram就是一个长度为N的词语组成的序列。 简单的流程 N-Gram 算法具体过程： 过滤掉文本数据中的标点符号和其他特殊字符； 对所有单词执行小写转换，并删除单词之间的空格、换行符等标志位； 使用长度为 N 的窗口对文本内容执行字符级滑动取词，将结果存入有序列表。 这里直接借鉴了别人的代码，看的懂就行 12345678910111213141516171819202122232425262728293031323334353637def text_filter(text: str) -&gt; str: \"\"\" 文本过滤器：过滤掉文本数据中的标点符号和其他特殊字符 \"\"\" result = str() for t in text: if t.isalnum(): if t.isalpha(): t = t.lower() result += str(t) return resultdef slide_word(text: str, l: int = 5) -&gt; list: \"\"\" 滑动取词器 Input: text='abcd',l=2 Output: ['ab','bc','cd'] :param text: 过滤后的文本 （只包含小写数字/字母） :param l: 滑动窗口长度，默认为 5 :return: \"\"\" tf = text_filter(text) result = list() if len(tf) &lt;= l: result.append(tf) return result for i in range(len(tf)): word = tf[i:i + l] if len(word) &lt; l: break result.append(word) return resultif __name__ == '__main__': banner = 'abcdefghigkLMN*^%$* \\r\\n)021' print(slide_word(banner)) 返回 1['abcde', 'bcdef', 'cdefg', 'defgh', 'efghi', 'fghig', 'ghigk', 'higkl', 'igklm', 'gklmn', 'klmn0', 'lmn02', 'mn021'] 隐马尔科夫模型算法 (HMM)通过模拟人对句子的理解，达到识别词的效果，基本思想是语义分析，句法分析，利用句法信息和语义信息对文本进行分词。自动推理，并完成对未登录词的补充是其优点。不成熟. 具体概念:有限状态机\\语法约束矩阵\\特征词库 以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)。 自动分词过程就是通过词表和相关信息来做出词语切分的决策。 与此相反，基于字标注的分词方法实际上是构词方法。 即把分词过程视为字在字串中的标注问题。 由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式： 123(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／(乙)字标注形式：上／B海／E计／B划／E N／S 本／s世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S 首先需要说明，这里说到的“字”不只限于汉字。 考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。 把分词过程视为字的标注问题的一个重要优势在于，它能够平衡地看待词表词和未登录词的识别问题。 在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。 在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。 在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。 最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。然而这一简单处理带来的分词结果却是令人满意的。 总结我估摸着我还是要好好把这些算法过一遍，未来还有很长的路要走，算法也太难了。。。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"},{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"},{"name":"算法","slug":"算法","permalink":"https://yemilice.com/tags/%E7%AE%97%E6%B3%95/"}],"keywords":[]},{"title":"一次Golang服务占用CPU过大的排查经过","slug":"一次Golang服务占用CPU过大的排查经过","date":"2020-08-17T01:51:47.000Z","updated":"2020-08-17T08:31:30.956Z","comments":true,"path":"2020/08/17/一次Golang服务占用CPU过大的排查经过/","link":"","permalink":"https://yemilice.com/2020/08/17/%E4%B8%80%E6%AC%A1Golang%E6%9C%8D%E5%8A%A1%E5%8D%A0%E7%94%A8CPU%E8%BF%87%E5%A4%A7%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E8%BF%87/","excerpt":"","text":"前言前阵子写了个ETCD选主的代码，持续后台执行，相安无事一阵我就干别的事儿去了，我寻思小爷虽然代码写的一般，但是不至于出错啊。 但是我想的实在是太单纯了，应了那句话，我还是too young啊，高估了自己的姿势水平，一下搞出来一个大新闻。周六大半夜告警在那里 biubiubiu 的往我邮箱里塞，我当时正在COD战场上挥汗如雨，手机的震动就像电动马达一样给我腿都快整麻了，一看邮箱，好家伙，CPU占用百分之200多！Cgroup都没给它限制住。 所以我寻思，咱就好好分析下到底怎么回事儿吧。 工具准备一般验证一个server详细的CPU和内存占用，Python我会选择PDB，C++我会选择GDB，但是GOlang这个就相对来说比较陌生，经过我查询资料（谷歌）过后得知，Golang有神奇pprof，还可以分析整个函数占用，这就很牛逼了，果断学习了一波之后上手。 pprof的简单使用首先pprof分为两个大包 123net/http/pprof runtime/pprof 如果你的go程序是用http包启动的web服务器，你想查看自己的web服务器的状态。这个时候就可以选择net/http/pprof。你只需要引入包_”net/http/pprof”。 例如 12345import _ \"net/http/pprof\"go func() &#123; http.ListenAndServe(\"0.0.0.0:8080\", nil)&#125;() 我现在要分析我的go server为什么占用了过大的CPU，所以，我需要在我的Go server的main中添加几行导入pprof的代码，一些业务代码我这里会直接隐去，请见谅 12345678910111213141516171819202122232425262728import ( _ \"net/http/pprof\" \"github.com/gin-gonic/gin\" ······)func main() &#123; // 这里使用了gin框架，模拟我的业务环境 gin.SetMode(gin.ReleaseMode) router := gin.Default() //ETCD选主的逻辑，这里很可能是持续占用CPU的罪魁祸首 go infimanage.Retry(infiapi.InitSyncStart) //调用pprof的逻辑，指定6161端口 go func() &#123; http.ListenAndServe(\":6161\", nil) &#125;() //这里是原本的服务接口,这里只是举个例子，模拟一波 .... router.POST(\"/synctask/add\", addwork, AddSyncWorks) router.GET(\"/synctask/del\", DeleteSyncWorks) s := &amp;http.Server&#123; Addr: \":8788\", Handler: router, ReadTimeout: 10 * time.Second, WriteTimeout: 10 * time.Second, &#125; _ = s.ListenAndServe()&#125; 现在走一波 1go run main.go 现在server就启起来了，然后咱们要来一波分析首先指定监控main server 30s 1go tool pprof http://127.0.0.1:6161/debug/pprof/profile -seconds 30 会冒出下面一大堆东西，就相当于进入了pprof，类似pdb，gdb的调试shell下 123456Fetching profile over HTTP from http://localhost:6161/debug/pprof/profile?seconds=30Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.007.pb.gzType: cpuDuration: 1mins, Total samples = 26.55s (44.15%)Entering interactive mode (type \"help\" for commands, \"o\" for options)(pprof) 获取CPU占比前10的函数 123456789101112131415(pprof) top10Showing nodes accounting for 25.92s, 97.63% of 26.55s totalDropped 85 nodes (cum &lt;= 0.13s)Showing top 10 nodes out of 21 flat flat% sum% cum cum% 23.28s 87.68% 87.68% 23.29s 87.72% syscall.Syscall 0.77s 2.90% 90.58% 21.77s 80.90% runtime.selectgo 0.58s 2.18% 92.77% 0.58s 2.18% runtime.mcall 0.53s 2.00% 94.76% 1.42s 5.35% runtime.timerproc 0.36s 1.36% 96.12% 30.39s 98.47% go.etcd.io/etcd/clientv3/... 0.35s 1.32% 97.44% 0.45s 1.69% runtime.greyobject 0.02s 0.075% 97.51% 24.96s 94.01% main.main.func1 0.01s 0.038% 97.55% 23.91s 90.06% os.(*File).Write 0.01s 0.038% 97.59% 0.19s 0.72% runtime.mallocgc 0.01s 0.038% 97.63% 23.30s 87.76% syscall.Write 这一波就能看出来了，到底谁才是占用CPU过大的罪魁祸首，看到了么，有个select和etcd的keeplive，但是这样似乎还不那么直观，这时候你需要一个叫做火焰图的东西 pprof输出火焰图安装输出火焰图的server安装配置FlameGraph 1git clone https://github.com/brendangregg/FlameGraph.git 配置FlameGraph 1PATH=$PATH:/rootg/go/src/github.com/brendangregg/FlameGraph 配置go-torch，这是生成火焰图的必备工具 1go get -v github.com/uber/go-torch 生成火焰图确保都安装完成了之后 直接执行命令 此时你要确保第一步那些监控代码已经写入 1go-torch -u http://127.0.0.1:6161/debug/pprof/ -p &gt; cpu-local.svg 这里会生成一个svg，这个东西可以用浏览器打开，打开是这样的 看到没，这一下就暴露出来了，谁占比多，你还可以点进去看详细，比如 这下可以定位了，就是选主逻辑当中的select或者是for循环出了问题 解决问题我们回头去反查ETCD的选主逻辑代码 123456789101112131415161718192021//ElectMasterNode 争抢主节点服务func ElectMasterNode() error &#123; ips, err := GetNodeIp() if err != nil &#123; return err &#125; etcd, err := infidb.New() if err != nil &#123; return err &#125; for &#123; _ = etcd.Newleaseslock(ips) &#125;&#125;//Masterwork 选主尝试func Masterwork() &#123; select &#123; ElectMasterNode() &#125;&#125; 这里的问题就是，在select中重复调用了for循环，应该是出现了for死循环，导致了CPU持续被抢占，这边修改一下代码逻辑 12345678910111213141516171819//ElectMasterNode 争抢主节点服务func ElectMasterNode() error &#123; ips, err := GetNodeIp() if err != nil &#123; return err &#125; etcd, err := infidb.New() if err != nil &#123; return err &#125; _ = etcd.Newleaseslock(ips)&#125;//Masterwork 选主尝试func Masterwork() &#123; select &#123; ElectMasterNode() &#125;&#125; 这样问题应该就完全解决了。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"出埃及记_普通程序员的买房之路","slug":"出埃及记-普通程序员的买房之路","date":"2020-08-10T00:56:30.000Z","updated":"2020-08-10T01:35:06.420Z","comments":true,"path":"2020/08/10/出埃及记-普通程序员的买房之路/","link":"","permalink":"https://yemilice.com/2020/08/10/%E5%87%BA%E5%9F%83%E5%8F%8A%E8%AE%B0-%E6%99%AE%E9%80%9A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E4%B9%B0%E6%88%BF%E4%B9%8B%E8%B7%AF/","excerpt":"","text":"出埃及记-前篇-买房理由有时候时间一晃，就过去了4年多，我2016年毕业，现在已经是2020年。 按道理说我也是老油条了，不比刚入行的小鲜肉了，但是依旧没个对象，也没个方向。说起买房，无非就是和兄弟朋友推杯换盏中听到的，谁又买了房，谁又结了婚罢了。 当房东再一次要我搬走的时候，我才觉得，有些时候你根本就没有选择的权力，因为人家的容错率很低，直接赔付你一个月房租即可，突然无比羡慕这些大城市中收取房租的人，有些时候会在加班的深夜反思，当年如果早买了房，是不是人生的选择就会多一点点？ 所以在搬了新家的第一个晚上，我躺在硬板床上，思绪似乎飞到九霄云外，我的亲人，朋友都在身边，但是睁开眼，只有我孤独一人空留在空白的房间徘徊罢了。 但是有两个字，我记住了，买房。 是啊，买房吧，那就现在就买吧。 出埃及记-中篇-买房过程选定了自己的购房需求，我就要根据自己的预算来选择房子。 首先需要研读一个比技术文档还要复杂的”限购/购房须知”。 绕来绕去我弄明白了一点，买房的区域是被严格限制了的。 说真的，我一直觉得这些限购的政策从来都是治不了本的，好比我在的高新区，不能够买旁边较为便宜的双流区的房子，只能去往买市中心和比较贵的天府新区的房子，哈哈，说白了，你我都懂得，有些人指定的政策完全都是为了消化指定区域的库存和大房子，正在建设的房子未来是哪里的韭菜接盘？ 像哥德巴赫猜一样，我大概猜出了我要买什么样的房子，奈何我的父母也不是大富大贵之人，手头预算有限，我大概也只能买个小房子去生活，并且这样就要掏空我的父母的大部分积蓄，我心里有些难受，一瞬间不想买房子了，但是母亲告诉我：房子，必须要买，买了房子，你才有家的感觉。 家的感觉？我从6岁开始就没有什么家的感觉了，我的父母很早就分开了，家对我来说永远都是孤独的水泥墙的存在，我更宁愿去相信母亲的意思是我不会被房东随便赶走，也有底气在这个大城市去生活吧。 手头资金不多，受限于政策，旁边的双流，更远龙泉地区，便宜的房子我是买不了的，我只能在南边去寻找合适的房子，是吧，那就买个合适的小房子吧？ 周六，周天跑了两天，我坐在滴滴上面，半眯着眼看着外面飞驰的树木，车辆，人流，看着远方立起的一座座高楼，会想着，我的家，会在哪里呢？这个钢筋水泥筑成的城市里，是否还有我的一席之地呢？ 出埃及记-后篇-未曾买房跑了两天，我还是拒绝了中介给我推荐的跃层，那个跃层很好，我很喜欢 但是他需要我自己将社保转移到双流/龙泉，然后进行别的操作 我听到哑然失笑，安得广厦千万间？当买个房子都这么麻烦的情况下，我有时候觉得自己就像个提线木偶，一直有人在掐住我的脖子而已。 不知道什么时候开始，没有以前那么有灵感，我一直不愿意让自己太懒，但是我有些时候有很多想说的，却堵在嘴边。 今天周一，我心情很复杂，未来，会变得更好吧？！","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://yemilice.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"keywords":[]},{"title":"Elasticsearch检索PDF和Office文档的方案测评","slug":"Elasticsearch检索PDF和Office文档的方案测评","date":"2020-07-29T06:02:32.000Z","updated":"2020-07-29T08:25:59.815Z","comments":true,"path":"2020/07/29/Elasticsearch检索PDF和Office文档的方案测评/","link":"","permalink":"https://yemilice.com/2020/07/29/Elasticsearch%E6%A3%80%E7%B4%A2PDF%E5%92%8COffice%E6%96%87%E6%A1%A3%E7%9A%84%E6%96%B9%E6%A1%88%E6%B5%8B%E8%AF%84/","excerpt":"","text":"前言这段时间主要攻关了一下Elasticsearch的一些特性，发觉Elasticsearch还是个挺牛逼的玩意儿，我以前经常用它存日志，还没想着拿来存别的东西，一般不都SQL嘛，但是我看了一下现在流行的检索方案，基本都是elasticsearch做检索引擎，说明这个东西经历了时间的考研，用了的人都说好。 我以前拿Elasticsearch来存储日志，搜东西相当方便，定义好检索语句直接走你，但是我看百度文库搜索，还可以做到docx，pdf关键字检索并且高亮，这个就很厉害了，这两天紧急攻关了一下，把自己踩的坑和收获记录下来，方便未来自己复习/查看 需求分析老样子，写任何blog，我都要做需求分析，这样才能更好的进行开发和写作。 我们要干嘛我们要实现一个elasticsearch检索pdf和office家族文件的功能，并且要对pdf/office文件进行一个全文检索，也就是说，搜索“Yemilice”，不仅仅是标题含有“Yemilice”，内容含有“Yemilice”的文件也要检索出来，并且给人家高亮。 我们的工作环境底层环境 Centos7服务器 Elasticsearch的版本 6.3.2 现有的检索解决方案经过我的发力工作（Google/baidu）,现在市面上流行这么几种方案 Elasticsearch 官方插件 ingest-attachment 第三方开源服务 fscrawler 大数据平台 Ambari 接下来麻烦事儿就来了，这三个方案，到底选哪个？才能更那啥呢，我倒不如把他们一一实现一遍，然后给大家伙儿展示一拨，然后最后再选一个合适的不就得了嘛！ 没事儿，苦了我一个，幸福大家伙儿呗！ 1. ingest-attachment插件这玩意儿是什么呢，说白了就是elasticsearch官方给大家贡献的一个插件，支持你把docx，pdf之类的东西导入到es当中。当然要你自己手动去实现接口。 1.1 ingest-attachment插件的安装首先，我认为你现在在CN境内，你就不要看网上那些老哥教的直接执行 1./elasticsearch-plugin install ingest-attachment 这样你会等到地老天荒也下载不完，现在你需要的就是下个离线包，然后再去安装 下载离线包的网址 1https://artifacts.elastic.co/downloads/elasticsearch-plugins/ingest-attachment/ingest-attachment-6.3.2.zip. 注意一点啊，一定要注意！你的ingest-attachment版本必须和你的elasticsearch一致，你要强行不听我的，到时候费心思整完，弄不好，那你就只能来颗华子再来一次了。 安装离线包 1./elasticsearch-plugin install ingest-attachment-6.3.2.zip 没报错就说明你安上了，不放心的话 1./elasticsearch-plugin list 看一下，有这个插件了说明你已经有了这个插件 1.2 ingest-attachment插件的基础使用看了下官网，其实他的逻辑很简单，整个管道，你把你的pdf/doc什么的给转成base64，然后通过管道把base64传进去，es会把所有的东西给你安排好。 我现在简单的整了个pdf，名字就叫1.pdf，现在开始吧。 抽取管道的函数编写，这里是告诉es，创建一个抽取管道pipeline 12345678910111213curl -X PUT \"localhost:9200/_ingest/pipeline/attachment\" -d '&#123; \"description\" : \"Extract attachment information\", \"processors\":[ &#123; \"attachment\":&#123; \"field\":\"data\", \"indexed_chars\" : -1, \"ignore_missing\":true &#125; &#125;, &#123; \"remove\":&#123;\"field\":\"data\"&#125; &#125;]&#125;' 创建一个index（表），名字叫pdf，设定1个分片（单节点，当然你自己可以改） 12345678910curl --location --request PUT '127.0.0.1:9200/pdf' \\--header 'Content-Type: application/json' \\-d '&#123; \"settings\": &#123; \"index\": &#123; \"number_of_shards\": 1, \"number_of_replicas\": 0 &#125; &#125;&#125;' 将pdf转为base64编码，然后上传到elasticsearch当中 这里网上那个方法是直接perl调base64，我这里一直报错，如果和我一样的老哥，用我下面给得另一种方法 方法1 1234curl --location --request PUT '10.0.7.234:9200/pdf/pdf/1?pipeline=attachment' \\--header 'Content-Type: application/json' \\-d '&#123; \"data\":\" '`base64 -w 0 /root/pdf/pdf.pdf | perl -pe's/\\n/\\\\n/g'`'\"&#125;' 方法2 直接导入base64码 1234curl --location --request PUT '10.0.7.234:9200/pdf/pdf/1?pipeline=attachment' \\--header 'Content-Type: application/json' \\-d '&#123; \"data\":\"JVBERi0xLjcNCiW1tbW1DQoxIDAgb2JqDQo8PC9UeXBlL0NhdGFsb2cvUGFnZXMgMiAwIFIvTGFuZyh6aC1DTikgL1N0cnVjdFRyZWVSb290IDE1IDAgUi9NYXJrSW5mbzw8L01hcmtlZCB0cnVlPj4vTWV0YWRhdGEgMzMgMCBSL1ZpZXdlclByZWZlcmVuY2VzIDM0IDAgUj4+DQplbmRvYmoNC......（这里我不展示完了）\"&#125;' 现在去查看index（表） 显示了作者，之类的杂七杂八信息，而content就是详细的全文，可以直接搜索，因为有我的大名我就马赛克了。。 这就导入了，搜索也是按一般的搜索方法 123456789GET pdf/_search&#123; \"query\": &#123; \"match\": &#123; \"attachment.content\": \"pdf\" &#125; &#125;&#125; 1.3 ingest-attachment插件的性能评测测试导入PDF（10M大小，无图片） base64命令在perl中执行了大概2s左右， 并且CPU/内存 在转换-&gt;存储这个阶段，占用了 可见在抽取文本 ——&gt; 传输存储入es的时候，插件可能会占用少部分CPU 测试导入PDF（40M大小，图片/文字混用） base64命令在perl中执行了大概10s左右，并且还返回了错误（perl长度过大的错误） 并且CPU/内存 在转换-&gt;存储这个阶段，占用了 在图片/文字混用并且pdf文件比较大的情况下，内存占用较大，并且返回较慢，这里需要注意，并且解析的base64会直接塞到系统内存当中，如果做多文件抽取，可能会有CPU/内存占用较大的情况出现。 1.4 ingest-attachment插件的优缺点优点： 安装方便，只需要下载zip安装包，调用elasticsearch本身自带的安装模块即可 使用方便，编辑一条管道抽取命令，可以直接利用linux的base64命令转码存入elasticsearch中 支持的格式比较多，支持ppt，pdf，doc，docx，xls等office常用的办公软件格式导入到elasticsearch当中。 导入后的文档可以直接输入中文进行检索 缺点： 大文件支持不够，如果超过100M的pdf，base64转码将比较缓慢，并且，存入到elasticsearch中的数据也十分庞大 必须要将文件下载到本地，然后必须进行base64转码才可以存到elasticsearch中，等于说，必须要实体文件，因为需要base64转码，如果文件在对象存储/云上，那就不可以这么操作了 对于pdf，doc中含有图片的情况，没有办法将图片中文文字识别出来，而且如果出现图片较多的情况，转码较慢，含有特殊字符的base64码导入也会无法识别。 1.5 ingest-attachment插件的使用场景 存储的文件不那么大的情况 存储的文件大部分都是纯文字的情况 存储的文件全文搜索精准度要求不那么高 2. fscrawler服务一个类似filebeat的监控服务，监控某个文件夹下面的文档，定时去遍历一次，如果发现了新添加的文档则会直接写入到elasticsearch当中。 2.1 fscrawler安装首先非常重要的一点，确定你的elasticseach版本， 如果你的版本 &lt;= 6.3.2，那你需要下载2.5 版本的fscrawler，这个非常重要，否则是无法启动任务的！！！！ 1https://repo1.maven.org/maven2/fr/pilato/elasticsearch/crawler/fscrawler/2.5/fscrawler-2.5.zip 如果你的版本 &gt; 6.3.2, 你就可以随意选择2.6，2.7的fscrawler了，区别还是有一些的，后面我会说。 下载下来，解压，这个你肯定会 1unzip fscrawler-2.5.zip 尝试跑一下 1./fscrawler 输出 12303:53:39,425 INFO [f.p.e.c.f.c.FsCrawler] No job specified. Here is the list of existing jobs:03:53:39,433 INFO [f.p.e.c.f.c.FsCrawler] [1] - work0103:53:39,433 INFO [f.p.e.c.f.c.FsCrawler] Choose your job [1-1]... 说明能用 2.2 fscrawler的使用方法启动服务 查看一下文件结构 编写指定的json去进行服务监控/设置 现在导入一个1.pdf的文件 监控到fscrawler发生了变化 去查看elasticsearch，已经有了这个1.pdf的文件 2.3 fscrawler的配置文件解析（彩蛋）一般来说，咱们fscrawler的配置文件都是默认在/root/.fscrawler/_settings.json下的，但是，如果你的es版本高于6.3.2，你下载的fs版本不是2.5，那么你的settings文件将会是yaml格式的，不知道是不是在向filebeat看齐。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; // 工程名 \"name\" : \"work01\", \"fs\" : &#123; // 监控的文件夹 \"url\" : \"/tmp/es\", // 多长时间去扫描一次 \"update_rate\" : \"30s\", // 添加例外 \"excludes\" : [ \"*/~*\" ], \"json_support\" : false, // 将文件名作为ID \"filename_as_id\" : false, // 文件大小添加 \"add_filesize\" : true, // 同步删除 \"remove_deleted\" : true, \"add_as_inner_object\" : false, \"store_source\" : false, \"index_content\" : true, \"attributes_support\" : false, \"raw_metadata\" : true, \"xml_support\" : false, \"index_folders\" : true, \"lang_detect\" : false, \"continue_on_error\" : false, // ocr开启 \"pdf_ocr\" : true, \"ocr\" : &#123; // ocr英文 \"language\" : \"eng\" &#125; &#125;, \"elasticsearch\" : &#123; \"nodes\" : [ &#123; // esip \"host\" : \"127.0.0.1\", \"port\" : 9200, \"scheme\" : \"HTTP\" &#125; ], \"bulk_size\" : 100, \"flush_interval\" : \"5s\", \"byte_size\" : \"10mb\" &#125;, \"rest\" : &#123; \"scheme\" : \"HTTP\", \"host\" : \"127.0.0.1\", \"port\" : 8080, \"endpoint\" : \"fscrawler\" &#125;&#125; 2.4 fscrawler的性能评测首先走一波导入文件测试 导入1M纯文字的pdf 瞬间就完成了，速度极快 elasticsearch监控信息如下 Fscrawler占用CPU/内存如下 导入10M纯文字的pdf 瞬间就完成了，速度极快 elasticsearch监控信息如下 Fscrawler占用CPU/内存如下 导入40M文字图片都有的pdf 速度很快，但是CPU占比一下就上去了，es还是没有什么变化 elasticsearch监控信息如下 Fscrawler占用CPU/内存如下 2.5 fscrawler的优缺点fscrawler我其实是比较推荐的 优点 支持的格式很多，并且自带OCR，如果内存/CPU富裕的情况可以直接上OCR，识别率虽然不是特别高但也够用了。 导入不用自己动手，设置好配置之后，fscrawler会帮你创建好index，然后自动导入文件 支持文件过滤，如果监控的文件夹里面闲杂文件比较多可以写正则过滤掉 CPU/内存占比比较低，我是用的自己的虚拟机，导入40Mpdf完全无问题，CPU占比上去了一瞬间就降下来了。 缺点 结合我们的使用场景，我们的文件在远端，需要下载下来，下载到指定文件夹，但是下载到指定文件夹后，fscrawler不会马上导入，是去定时扫描，下载到本地的话可能会把系统盘内存撑满，加快扫描时间会让CPU占比上升 安装部署比较麻烦，需要自己写service，自己写维护 配置文件需要自己定义，容错有问题，如果es down机，原有的文件发送一遍失败之后，不会继续发送 3. Ambari服务这个有个很奇葩的点，是要Es去依赖它，而不是它依赖Es，这东西本身是做大数据的！我感觉咱都有Es了还要啥自行车！ 这个是做大数据用的，安装一个大概1个多G，但是功能比较全面，支持各种大文件导入，还支持图片文字的读取，自动分词之类的，其实还是很好用。 3.1 使用一波AmbariAmbari的安装就不在这里介绍了。。这个安装比较复杂，但是网上大部分都是docker集成的，我们现在的环境没有docker，所以我只有手动安装了，在我的机器已经安上了，现在需要安装Ambari的Elasticsearch插件 1wget https://community.hortonworks.com/storage/attachments/87416-elasticsearch-mpack-2600-9.tar.gz 安装mpack 1ambari-server install-mpack --mpack=/path/to/87416-elasticsearch-mpack-2600-9.tar.gz --verbose 重启ambari-server 1ambari-server restart 在ambari-server 上部署elasticsearch 1curl --user admin:admin -i -H 'X-Requested-By: ambari' -X GET \"http://ambari.server:8080/api/v1/clusters/$cluster_name/configurations?type=cluster-env\" 启动elasticsearch 1systemctl restart elasticsearch 现在可以在Ambari的管理页面看到es服务器了 设置ambar的配置文件 123456789101112131415161718my-files: depends_on: serviceapi: condition: service_healthy image: ambar/ambar-local-crawler restart: always networks: - internal_network expose: - \"8082\" environment: - name=my-files - ignoreFolders=**/ForSharing/** - ignoreExtensions=.&#123;exe,dll,rar&#125; - ignoreFileNames=*backup* - maxFileSize=15mb volumes: - /media/Docs:/usr/data 现在尝试导入一个PDF 1234curl -X POST \\ http://ambar/api/files/Books/1984-george_orwell.pdf \\ -H 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \\ -F 1984-george_orwell.rtf=@1984-george_orwell.pdf 这下就完成了导入 3.2 高级特性监控S3对象存储 设定ak，sk 12echo ACCESS_KEY_ID:SECRET_ACCESS_KEY &gt; ~/.passwd-s3fschmod 600 ~/.passwd-s3fs 挂载s3 bucket 1mkdir /mnt/s3-bucket 添加挂载到fstab中 1mybucket /mnt/s3-bucket fuse.s3fs _netdev,allow_other 0 0 修改配置文件 123456789101112131415161718my-files: depends_on: serviceapi: condition: service_healthy image: ambar/ambar-local-crawler restart: always networks: - internal_network expose: - \"8082\" environment: - name=my-files - ignoreFolders=**/ForSharing/** - ignoreExtensions=.&#123;exe,dll,rar&#125; - ignoreFileNames=*backup* - maxFileSize=15mb volumes: - /media/Docs:/mnt/s3-bucket OCR文件识别 ambari自带强势的ocr识别，可以识别多种文字/字母/格式的文件，但是需要富裕的CPU和内存 3.3 可能会出现的问题优点 它可以很好地处理大文件（&gt; 100 MB） 它从PDF中提取内容（即使格式不佳并带有嵌入式图像），并对图像进行OCR 它为用户提供了简单易用的REST API和WEB UI 部署非常容易（感谢Docker） 它是根据Fair Source 1 v0.9许可开源的 开箱即用地为用户提供解析和即时搜索体验。 缺点 部署可能会相当麻烦，没有docker部署，单独部署的话配置比较复杂 OCR会占用大量的CPU/内存 如果不是大数据，有些大材小用了 4. 总结我都分析成这样了，该用哪个，心里是不是有点数了，其实很简单 当你的pdf/doc文件不大的情况下，上ingest-attachment插件 当你的文件经常发生变动，上fscrawler 当你是大数据老哥，上Ambari 今天的文章就到这，憋了个大的，如果帮到你，我很开心。 end 2020-07-29","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"},{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]},{"title":"Grow_Up","slug":"Grow-Up","date":"2020-07-28T06:38:28.000Z","updated":"2020-07-28T07:10:11.825Z","comments":true,"path":"2020/07/28/Grow-Up/","link":"","permalink":"https://yemilice.com/2020/07/28/Grow-Up/","excerpt":"","text":"前言有时候觉得自己快要撑不下去 但是依旧不能放弃 为了爸妈 为了自己 为了所有人 ONE温热的咖啡升腾起轻薄的水雾 满屏未敲完的代码依旧在提醒 忙碌和疲惫的一天即将结束 next，即将来到下一个季节 坐上地铁戴上耳机这是属于我的时间 把今天的不堪和疲惫就在此丢失 曾今的挚友们一一远去 他们很多人已把我超越 盲目的赚钱和生活已经让我麻木 也曾今质疑过自己走过的路 但是选择只能让我勇往直前 脚踏实地是我的人生格言 等到积累更多，得到更多的时候 我才能就此歇下，远离喧嚣 TWO不知不觉我已经长大 回想自己经历过的一切 人生只有努力一条路没有捷径 无论如何也会勇敢的深入虎穴 因为要成为能值得被铭记的人物 so what 成功或者失败 就让我燃尽自己所有的能量 然后像灰烬一样洒向远方 挥洒血汗，我是吞噬金钱的野兽 抢风头从来不是我的style脚踏实地才是 和时间之神做个交易吧 将努力和汗水换为当票 典当时间，预支痛苦， 收获的是快乐和幸福 END其实我写这些往往都是DEMO 一般还要做beat，但是我最近实在是没时间写 我有时候挺后悔自己没和自己的兄弟去做说唱 但是世界上是没有后悔药的 现在就动起来吧。","categories":[],"tags":[{"name":"说唱之路","slug":"说唱之路","permalink":"https://yemilice.com/tags/%E8%AF%B4%E5%94%B1%E4%B9%8B%E8%B7%AF/"},{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://yemilice.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"keywords":[]},{"title":"使用Golang的gRPC框架的一点随想","slug":"使用Golang的gRPC框架的一点随想","date":"2020-07-07T05:32:21.000Z","updated":"2020-07-07T08:20:17.510Z","comments":true,"path":"2020/07/07/使用Golang的gRPC框架的一点随想/","link":"","permalink":"https://yemilice.com/2020/07/07/%E4%BD%BF%E7%94%A8Golang%E7%9A%84gRPC%E6%A1%86%E6%9E%B6%E7%9A%84%E4%B8%80%E7%82%B9%E9%9A%8F%E6%83%B3/","excerpt":"","text":"前言（写文章的原因）最近开发项目太重了，我有些感觉自己状态不太对，想要去换一个地方生活，但是说到要做的事就一定要做，学习是不能停下来的。 最近开发使用了一下gRPC，不同于以前我自己写的Python 的 RPC，gRPC相对来说用起来更简单，写起来更容易，可能一开始有点不好使，其实熟悉了也还是不错的，那么，这篇文章主要就写一下，gRPC的基础使用和在我项目中的实际应用吧。 RPC框架是什么简单说下什么是RPC，RPC是什么，全称是Remote Procedure Call Protocol，中文名称是远程调用协议，这个就说的很明白了 RPC干嘛的，远程调用的呗！ 远程调什么？参数呗！函数呗！发消息呗！ 大白话来说，像调用本地服务一样调用远程服务，就是你现在要完成一个动作，你需要一个老哥（机器）和你联动一起完成，所以你就要通过一个渠道告诉他，说，老铁，我们要一起做这个东西，知道吗。 这个渠道，就叫RPC，一般我们在网络服务，分布式服务，微服务中都需要这个东西。 从技术的角度上来说，RPC更类似于一种client，server的通信过程，client发送消息给server，server接收到消息，然后返回消息给client。 gRPC是什么我是在写Golang网络服务的时候发现了这个框架gRPC，一看就是谷歌亲儿子框架，粗看了一下，感觉其实还好，所以我仔细看了一下这个框架 首先这货需要一个ProtoBuf序列化工具，是给客户端提供接口的，ProtoBuf这个东西需要另外安装，没他就不能解析接口，这个的好处是，你的开发语言不受这个影响，相当于就是翻译器，能把多种语言编写的接口api翻译出来，给gRPC去用 第二就是需要gRPC的本体，这个如果你有Golang环境直接go get 即可 除此之外，粗看一下gRPC，其实也需要client和server端，也是互相交互的。 gRPC主要的好处这货的好处 其实说白了，就是性能高，从Golang的开发来说，就表明了gRPC走的也是高性能的逻辑。 protobuf这东西，也是gRPC钦定的接口编写工具，这东西可以严格的约束住接口，告诉他，你可以传什么，不能传什么，加强了gRPC的安全机制 protobuf可以压缩编码为二进制，二进制你们懂吧，这个东西就更快了，耍起来那就是大杀器，首先相比大刀，他就是手枪，不仅小，威力还大，还可以达到kill 人的目的，因为压缩成二进制，传递的数据量也小了，通过基础的http2还可以实现异步请求，这就很nice了。 来玩玩gRPC吧安装protobuf首先你要先去protobuf的大本营 https://github.com/protocolbuffers/protobuf/releases，先找对应的版本下载 我这里是Centos7 64位， 所以我选了protoc-3.12.3-linux-x86_64.zip 下载，你们要去找对应的版本下载。 然后你需要解压 1unzip protoc-3.12.3-linux-x86_64.zip CD到目录, 进到目录的bin下，给protoc赋上可执行的权限 12cd protoc-3.12.3-linux-x86_64/binchmod 777 protoc 这下protoc可用了，你应该把它移动到bin下，方便未来调用 1cp protoc /usr/bin 定义一个protobuf接口重头戏来了，现在我们从零开始开发了，现在我们先创建一个项目文件夹hellogRPC,这下面现在什么也没有。 看图 现在我们创一个名字叫manage的文件夹，在下面创建一个名为manage.proto的接口定义文件，类似这样 看图 现在开始编写proto文件 123456789101112131415161718192021syntax = \"proto3\";option objc_class_prefix = \"HLW\";package manage;//定义服务service GreeterServer &#123; // 定义一个具体的接口，定义输出和输出的样式 rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// 定义输入 stringmessage HelloRequest &#123; string name = 1;&#125;// 定义输出 stringmessage HelloReply &#123; string message = 1;&#125; 这里具体的意思就是，我，下一道圣旨，以后，都按着这个给我输入输出，现在我只开了一个sayhello的接口，别的进不来，就这样。 编译protobuf文件这一步是要把protobuf生成为二进制文件，就像我们前面说的，这也是人家gRPC的一个有点，就是有点儿操蛋，因为麻烦。。。。 1go get -u github.com/golang/protobuf/protoc-gen-go 下下来之后，你还得把他移动一下下，到你的go path下面，我这里是/root/go 12cd /root/go/bincp -r protoc-gen-go /usr/bin 再编译一下，走一波编译 1protoc --go_out=plugins=grpc:. *.proto 这下你可以看见，多出一个同名得pb.go文件，类似这样 这就编译成功了。下一步继续干活儿 编写client服务client相当于发号施令的大哥，主要就是去找server，告诉server，你们要干嘛，要干嘛这种。 咱们现在创建一个client.go文件 现在开写 123456789101112131415161718192021222324252627282930package mainimport ( \"log\" pb \"/你的目录/hellogRPC/manage\" \"golang.org/x/net/context\" \"google.golang.org/grpc\")//HelloWork 定义一个hello world 方法func HelloWork(ip string, name string) &#123; conn, err := grpc.Dial(ip, grpc.WithInsecure()) if err != nil &#123; log.Fatalf(\"did not connect: %v\", err) &#125; defer conn.Close() c := pb.NewGreeterClient(conn) r, err := c.SayHello(context.Background(), &amp;pb.HelloRequest&#123;Name: name&#125;) if err != nil &#123; log.Fatalf(\"could not greet: %v\", err) &#125; log.Printf(\"Greeting: %s\", r.Message)&#125;func main() &#123; //传输要发送的ip，传输name HelloWork(\"127.0.0.1:4321\", \"work\")&#125; 编写server服务server服务就类似于等待发号施令的小弟，他们会常驻在后台等待，并且会开启一个专用端口去接受client发来的信息，类似这样 12345678910111213141516171819202122232425262728293031323334package mainimport ( \"log\" \"net\" pb \"/你的目录/hellogRPC/manage\" \"golang.org/x/net/context\" \"google.golang.org/grpc\")const ( //定义一个端口 port = \":4321\")// 定义servertype server struct&#123;&#125;//定义SayHello 记住，这里的SayHello要和你proto里面的接口名一致func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123; return &amp;pb.HelloReply&#123;Message: \"Hello \" + in.Name&#125;, nil&#125;func main() &#123; lis, err := net.Listen(\"tcp\", port) if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err) &#125; s := grpc.NewServer() pb.RegisterGreeterServer(s, &amp;server&#123;&#125;) s.Serve(lis)&#125; 来跑一下先启动 server.go 1go run server.go 再启动 client.go 1go run clinet.go 发现输出了 12020/07/07 16:01:10 Greeting: Hello work 这时我们的目录结构如下，基础的gRPC服务也就此完成。 总结其实这个还是比较简单的，但是具体到后面使用，你可能会遇到，gRPC和ETCD冲突，编译的时候出现bug，编译的时候编译器版本对不上等等等等，我这次踩了不少坑，不过还好过来了，项目快要结束了，终于要结束了，我想我需要休息一下，然后去找一个适合我的job。 预告一下，未来我将进行kail linux 和渗透测试的文章更新，基础的技术我应该不会再更了，毕竟我15年入行，16年正式上班，是该有一些蜕变了，未来我将持续输出高质量的文章，请多多期待吧。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"},{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]},{"title":"恶性循环","slug":"恶性循环","date":"2020-06-17T04:43:19.000Z","updated":"2020-06-17T05:05:09.308Z","comments":true,"path":"2020/06/17/恶性循环/","link":"","permalink":"https://yemilice.com/2020/06/17/%E6%81%B6%E6%80%A7%E5%BE%AA%E7%8E%AF/","excerpt":"","text":"前言改 不 了 写 前 言 的 毛 病 这 个 月 心 情 很 不 美 丽 来 自 心 底 的 呐 喊 释 放 出 深 渊 的 野 兽 one我 得 了 名 为 平 庸 的 不 治 之 症 就 让 我 在 灰 暗 的 地 底 死 去 双 手 合 十 祈 求 神 明 的 营救 看 不 到 的 希 望 的 祷 告 不 会 得 到 回 应 无 尽 的 在 深 渊 中 反 复 轮 回 建 起 一 座 名 为 失 败 者 的 宫 殿 不 要 阻 止 我 不 要 拯 救 我 我 根 本 无 药 可 医 就 算 我 一命 呜 呼 我 也 要 夺 走 你 的 一切 这 是 我 的 战 争 宣 言 匹 夫 的 怒 火 只会 萦 绕 在 你 身 上 在 战 争 的 前 线 树 立 起 失 败 者 的 旗 帜 这 就 是 失 败 者 的 战 争 宣 言 two承 认 自 己 平 庸 不 如 让 我 放 弃 自 己 我 出 生 的 那 一 瞬 间 就 证 明 我 要 在 这 个 世 间 留 下 痕 迹 就 算 我 得 了 不 治 之 症 也 不 要 怀 疑 我 要 成 为 坐 拥 一 切 的 人 恶 性 循 环 只 会 让 我 越 来 越 恶 因 为 我 是 来 自 地 狱 的 恶 鬼 不 会 向 任 何 人 低 头 包 括 上 帝 双 手 合 十 可 他 并 没 有 出 现 虽 然 我 活 成 这 样 子 可 是 我 依 旧 不 赖 就 算 我 一 命 呜 呼 也 要 记 得 把 钱 都 给 我 把 鼓 吹 战 争 的 传 单 散 发 各 地 这 就 是 我 的 宣 战 宣 言 高 高 举 起 梦 想 成 真","categories":[],"tags":[{"name":"说唱之路","slug":"说唱之路","permalink":"https://yemilice.com/tags/%E8%AF%B4%E5%94%B1%E4%B9%8B%E8%B7%AF/"},{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://yemilice.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"keywords":[]},{"title":"我的自我介绍","slug":"我的自我介绍","date":"2020-06-02T07:05:31.000Z","updated":"2020-06-02T08:20:43.249Z","comments":true,"path":"2020/06/02/我的自我介绍/","link":"","permalink":"https://yemilice.com/2020/06/02/%E6%88%91%E7%9A%84%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"我的自我介绍hello, 这是我， 你可能不认识我， 那么， 现在由我来进行自我介绍。 do it。 我 叫 Y k，你 可 以 叫 我 l k，来 自 c d c 出 生 在 钟 鸣 鼎 食 之 家，其 他 都 是 secret 有 个 很 爱 我 的 妈 妈，更 多 一 点 不 能 告 诉 你 ok，ok，让 我 们 继 续 下 去 以 前 是 个 Coder 现 在 依 旧 是 Coder 兼 职 玩 个 Rap 但 是 绝 不 是 Faker 从 来 不 穿 Aj but 我 只 爱 air force 嫉 妒 我 的 hater 我 从 来 不 care about 真 正 的 键 盘 侠 用 键 盘 当 做 武 器 project 像 拿 了 绿 卡 一 路 run 到 底 把 项 目 奖 金 和 fake coder 全 扫 进 包 包 里 从 不 追 逐 金 钱 因 为 大 家 都 会 show me the money 不 会 过 多 犹 豫，代 码 就 像 killer shoot you body Github 上 又 多 了 几 颗 星 星，荣 誉 勋 章 记 录 总 不 停 堆 积 成 山 压 力 从 不 畏 惧 ，制 定 策 略 就 像 指 挥 沙 盘 游 戏 运 筹 帷 幄 将 代 码 握 在 手 里 ，因 为 我 是 coder 届 的 汤 姆 克 兰 西 没 有 天 分 我 需 要 更 加 努 力， 感 谢 你 来 我 场 地 看 到 这 里 照 顾 不 周 随 便 看 看 有 意 见 就 提，回 头 去 你 场 子 battle 乐 此 不 疲。","categories":[],"tags":[{"name":"说唱之路","slug":"说唱之路","permalink":"https://yemilice.com/tags/%E8%AF%B4%E5%94%B1%E4%B9%8B%E8%B7%AF/"}],"keywords":[]},{"title":"Python写一个自动点餐程序","slug":"Python写一个自动点餐程序","date":"2020-06-02T02:17:12.000Z","updated":"2020-07-07T08:20:12.479Z","comments":true,"path":"2020/06/02/Python写一个自动点餐程序/","link":"","permalink":"https://yemilice.com/2020/06/02/Python%E5%86%99%E4%B8%80%E4%B8%AA%E8%87%AA%E5%8A%A8%E7%82%B9%E9%A4%90%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"Python写一个自动点餐程序为什么要写这个公司现在用meican作为点餐渠道，每天规定的时间是早7：00-9：40点餐，有时候我经常容易忘记，或者是在地铁/公交上没办法点餐，所以总是没饭吃，只有去楼下711买点饭团之类的玩意儿，所以这是促使我写点餐小程序的原因。 点餐的流程登录 —&gt; 点餐 —&gt; 提交 哈哈，是不是很简单，其实这个还好，说白了，就是登录上去，然后拿到cookie，保持一个登录状态，然后再去点餐，点餐就是构造请求，发送到指定的点餐URL上就可以了。 登录首先我们点开 https://meican.com/ 上面要求我们登录，我们这里输入自己的账号密码，登录上去之后可以看见一个请求. 这个请求就是登录的请求，我们看下需要传什么参数，然后我们去完全构造这个请求，也就是参数一致，并且带浏览器头,这里我们也需要去保存cookie，也就是说，我们需要自己的账号时刻保持online状态，所以需要保存cookie，需要时候调用 所以我们需要实现如下功能 登录请求构造 保持登录状态 保存cookies 使得后来的访问都带cookie 代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import jsonimport requestsimport http.cookiejar as HCsession = requests.session()session.cookies = HC.LWPCookieJar(filename='cookies')def login_meican(): \"\"\" 登录美餐，寻找cookie文件，没cookie文件就重新载入 :return: \"\"\" # 储存cookie作为日后使用，三天clear一次 try: session.cookies.load(ignore_discard=True) except: print('未找到cookies文件') save_cookie()def save_cookie(): \"\"\" 如果没cookie，登录逻辑 :return: \"\"\" login_url = 'https://meican.com/account/directlogin' # Headers hearsers = &#123; \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\", \"Referer\": \"https://meican.com/login\", \"Origin\": \"https://meican.com\", \"Host\": \"meican.com\", \"Accept\": \"*/*\" &#125; # Login need data data = &#123; \"username\": \"xxxxxxxxxxx\", \"loginType\": \"username\", \"password\": \"xxxxxxxxxxx\", \"remember\": \"true\" &#125; try: r = session.post(login_url, headers=hearsers, data=data) r.raise_for_status() session.cookies.save() except Exception as e: print(\"login error!\") return 0 上面的代码实现了登录。 点餐找到菜单这里需要找到菜单，因为截图忘了截，这里就直接公布吧，找到菜单需要两个参数，一个是uuid，另一个是addrid，也就是你登陆的凭证+你所在地区的id，没有这两个是无法找出菜单的，并且也无法继续点餐流程。 如何获得这两个参数在登录的时候我发现了一个URL，这个URL是 https://meican.com/preorder/api/v2.1/calendaritems/list?withOrderDetail=false&amp;beginDate=2019-09-04&amp;endDate=2019-09-04， 这个URL下的返回有我们要的参数，uuid 和 addrid，所以构造请求去获取这两个参数 123456789101112def get_for_my_order(): \"\"\" 找到usertorken, addrid :return: \"\"\" user_dict = &#123;&#125; Now_date = datetime.date.today() z = session.get(\"https://meican.com/preorder/api/v2.1/calendaritems/list?withOrderDetail=false&amp;beginDate=&#123;Now&#125;&amp;endDate=&#123;Now&#125;\".format(Now=Now_date)) x = json.loads(z.text) user_dict[\"uuid\"] = x[\"dateList\"][0][\"calendarItemList\"][0][\"userTab\"][\"uniqueId\"] user_dict[\"addrid\"] = x[\"dateList\"][0][\"calendarItemList\"][0][\"userTab\"][\"corp\"][\"addressList\"][0][\"uniqueId\"] return user_dict 构造获取菜单请求找到获取菜单的URL https://meican.com/preorder/api/v2.1/recommendations/dishes?tabUniqueId={uuid}&amp;targetTime={Now}+09:40 这里需要一个参数uuid，调取我们获取参数的函数 1234567891011121314151617def get_menu(): \"\"\" 获取餐单逻辑 :return: \"\"\" menu_dict = &#123;&#125; menu_list = [] Now_date = datetime.date.today() uuid = get_for_my_order()[\"uuid\"] z = session.get(\"https://meican.com/preorder/api/v2.1/recommendations/dishes?tabUniqueId=&#123;uuid&#125;&amp;targetTime=&#123;Now&#125;+09:40\".format(uuid = uuid, Now=Now_date)) menu = json.loads(z.text)[\"myRegularDishList\"] for i in menu: menu_dict[\"id\"] = i[\"id\"] menu_dict[\"name\"] = i[\"name\"] z = copy.deepcopy(menu_dict) menu_list.append(z) return menu_list 输出所有的菜单，以一个list作为输出 提交构造点餐请求首先先找到点餐的URL https://meican.com/preorder/api/v2.1/orders/add 查看点餐需要的参数： 12345678 data = &#123; \"corpAddressUniqueId\": addrid, \"order\": x, \"remarks\": y, \"tabUniqueId\": uuid, \"targetTime\":target_time, \"userAddressUniqueId\":addrid&#125; 构造点餐请求 123456789101112131415161718192021222324252627282930def order_action(): \"\"\" 点餐逻辑 :return: \"\"\" addrid = get_for_my_order()[\"addrid\"] uuid = get_for_my_order()[\"uuid\"] menu_list = get_menu() menu_id = choice(menu_list)[\"id\"] target_time = str(datetime.date.today()) + \" \" + \"09:40\" x = str([&#123;\"count\":1,\"dishId\":menu_id&#125;]) y = str([&#123;\"dishId\":menu_id,\"remark\":\"\"&#125;]) data = &#123; \"corpAddressUniqueId\": addrid, \"order\": x, \"remarks\": y, \"tabUniqueId\": uuid, \"targetTime\":target_time, \"userAddressUniqueId\":addrid &#125; headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\" &#125; try: z = session.post(\"https://meican.com/preorder/api/v2.1/orders/add\", headers=headers, data=data) z.raise_for_status() except: return \"点餐错误！\" 所用的知识点一览 Python requetst的post，session cookie的保存和调用 json的输出和浏览 random.choice 的列表元素随机选择 Python构造请求和登录逻辑","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang中Context使用的一点随想","slug":"Golang中Context使用的一点随想","date":"2020-06-01T02:18:36.000Z","updated":"2020-06-01T02:20:05.151Z","comments":true,"path":"2020/06/01/Golang中Context使用的一点随想/","link":"","permalink":"https://yemilice.com/2020/06/01/Golang%E4%B8%ADContext%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E7%82%B9%E9%9A%8F%E6%83%B3/","excerpt":"","text":"Golang中Context使用的一点随想前言这一篇是三巨头最后一篇了，前两篇介绍了channel，waitgroup，今天这篇来介绍一下context，相比于其他两种，我倒是更推荐context（上下文）这种控制goroutine的方法，为什呢，下面我就来详细的说一说吧。 什么是Context（逻辑上下文）Context是一种链式的调用逻辑，一般是用来控制goroutine，例如说，控制goroutine的启动，停止，暂停，取消等等。 我这里举一个简单的例子来说明Context怎么用 123456789101112131415161718192021222324252627282930313233//WorkDir 定义一个遍历目录逻辑，假设这个目录很大，但是咱们需要随时把它结束掉func WorkDir(ctx context.Context, name string) &#123; //假装这是一个大列表，很大很大那种 filePathList := []string&#123;........&#125; for _, file := range(filePathList) &#123; //假装这里处理一下目录, 打印一下目录 fmt.Println(file) select &#123; //检测到ctx发生了变化 case &lt;-ctx.Done(): fmt.Println(name,\"任务退出，骨灰都给你扬了\") return //否则什么都不做 default: &#125; &#125;&#125;func main() &#123; //定义一个waitcancel ctx, cancel := context.WithCancel(context.Background()) //将ctx作为参数传入函数当中 go WorkDir(ctx, \"三秒之内撒了你\") //让程序跑个几秒 time.Sleep(3 * time.Second) fmt.Println(\"该杀任务了\") //杀任务操作 cancel() //没有监控输出，就表示停止 time.Sleep(2 * time.Second) fmt.Println(\"任务让我杀了\")&#125; 怎么样，是不是很简单，简单来说，ctx就是传递的信号，你给每一层传递的ctx，不管传的再多，都只有一个爹 1ctx, cancel := context.WithCancel(context.Background()) cancel是来通知goroutine结束的，当爹ctx执行了cancel之后，就表示，我死了，你们得和我一起被株连，大家一起完蛋。 链式的意思就是，爹ctx是母体，它可以不断的继续下发产生多个子ctx，当cancel通知母体死亡的时候，子ctx也要跟着一起完蛋，所以链式就是，一荣俱荣，一关俱关的，cancel就是丧钟，调用就会释放所有的被感染（ctx下发）的goroutine。 Context的几个重要方法WithCancel方法WithCancel方法的接口 1func WithCancel(parent Context) (ctx Context, cancel CancelFunc) 这里会返回一个ctx和cancel，ctx是一个可以被拷贝的context，这个context可以作为父节点继续向下传递，cancel则是通知ctx退出的信号，调用CancelFunc的时候，关闭c.done()，直接退出goroutine。 举个简单的例子，算了我偷个懒，用上面的例子，ctrl c ctrl v一波 1234567891011121314151617181920212223242526272829303132//WorkDir 定义一个遍历目录逻辑，假设这个目录很大，但是咱们需要随时把它结束掉func WorkDir(ctx context.Context, name string) &#123; //假装这是一个大列表，很大很大那种 filePathList := []string&#123;........&#125; for _, file := range(filePathList) &#123; //假装这里处理一下目录, 打印一下目录 fmt.Println(file) select &#123; //检测到ctx发生了变化 case &lt;-ctx.Done(): fmt.Println(name,\"任务退出，骨灰都给你扬了\") return //否则什么都不做 default: &#125; &#125;&#125;func main() &#123; //定义一个waitcancel ctx, cancel := context.WithCancel(context.Background()) //将ctx作为参数传入函数当中 go WorkDir(ctx, \"三秒之内撒了你\") //让程序跑个几秒 time.Sleep(3 * time.Second) fmt.Println(\"该杀任务了\") //杀任务操作 cancel() //没有监控输出，就表示停止 time.Sleep(2 * time.Second) fmt.Println(\"任务让我杀了\")&#125; WithDeadline方法WithDeadline方法的接口 1func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) 这下机制出现了变化，cancel换成了deadline，参数也换了，换成了time，聪明的你应该想到了什么吧，这个函数其实是设置一个具体的死亡时间（deadline），当到达了指定的deadline时间之后，将所有的goroutine进行退出。咱们简单点儿，告诉你，就是到点儿退出。这里咱们照旧，举个例子，用例子来个详细说明。 123456789101112131415161718192021222324252627282930313233func WorkDir(ctx context.Context, name string) &#123; //假装这是一个大列表，很大很大那种 filePathList := []string&#123;........&#125; for _, file := range(filePathList) &#123; //假装这里处理一下目录, 打印一下目录 fmt.Println(file) select &#123; //检测到ctx发生了变化 case &lt;-ctx.Done(): fmt.Println(name,\"任务超时了，骨灰都给你扬了\") return case &lt;-time.After(1 * time.Second): fmt.Println(\"我还在......\") //否则什么都不做 default: &#125; &#125;&#125;func main() &#123; //定义一个WithDeadline，这里定义了一个具体的时间点 ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(10 * time.Second)) //将ctx作为参数传入函数当中 go WorkDir(ctx, \"从现在开始计时，10s之后撒了你\") //让程序跑个几秒 time.Sleep(3 * time.Second) fmt.Println(\"该杀任务了\") //杀任务操作 cancel() //没有监控输出，就表示停止 time.Sleep(2 * time.Second) fmt.Println(\"任务让我杀了\")&#125; 千万注意，后面还有个方法叫WithTimeout，它们两个说一样也不一样，WithTimeout是设置一个超时时间，WithDeadline是设定一个具体时间点，比如我上面那个，10s之内撒了goroutine，这个较为抽象，后面WithTimeout讲解的时候我会多费点功夫的。 WithTimeout方法WithTimeout方法的接口 1func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) 对比一下上面那个WithDeadline，看到没，都需要传入一个时间变量，但是还是那句话，上面的WithDeadline需要的是具体时间，下面的WithTimeout就简单多了，就需要一个时间就行了，说简单点，上面的WithDeadline，要的是一个具体时间，例如 “2020-01-01 12:00:00”， 1time.Now().Add(5*time.Second) 但是下面的WithTimeout，就很简单，就要一个超时时间就好了，例如5s 1timeout := 3 * time.Second 类似crontab的用法。好了，继续举个例子 123456789101112131415161718192021222324252627282930313233func WorkDir(ctx context.Context, name string) &#123; //假装这是一个大列表，很大很大那种 filePathList := []string&#123;........&#125; for _, file := range(filePathList) &#123; //假装这里处理一下目录, 打印一下目录 fmt.Println(file) select &#123; //检测到ctx发生了变化 case &lt;-ctx.Done(): fmt.Println(name,\"任务超时了，骨灰都给你扬了\") return case &lt;-time.After(1 * time.Second): fmt.Println(\"我还在......\") //否则什么都不做 default: &#125; &#125;&#125;func main() &#123; //定义一个WithDeadline，这里定义了一个具体的时间点 ctx, cancel:= context.WithTimeout(context.Background(), 5 * time.Second) //将ctx作为参数传入函数当中 go WorkDir(ctx, \"不管怎么样，没返回，5s之后撒了你\") //让程序跑个几秒 time.Sleep(20 * time.Second) fmt.Println(\"该杀任务了\") //杀任务操作 cancel() //没有监控输出，就表示停止 time.Sleep(2 * time.Second) fmt.Println(\"任务让我杀了\")&#125; 其实这两个用法都差不多，就是一个要求指定时间，一个随机时间，都是可以自己定义的. WithValue方法WithValue方法的接口 1func WithValue(parent Context, key interface&#123;&#125;, val interface&#123;&#125;) (Context) 这个看一下人家的传入，一个context，一个key, 一个接口interface，这明显就是一个key value类型的参数，这个方法的主要功能就是传递消息用，传递的元数据一般都是在子ctx中要使用到的，这个函数其实我用的不多，我参考了一下其他老哥的写法，总结了一下，具体的例子如下 1234567891011121314151617181920212223242526272829//定义一个keyvar key string = \"keywork\"func main() &#123; ctx, cancel := context.WithCancel(context.Background()) //附加值 valueCtx := context.WithValue(ctx, key, \"【监控1】\") go watch(valueCtx) time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second)&#125;func watch(ctx context.Context) &#123; for &#123; select &#123; case &lt;-ctx.Done(): //取出值 fmt.Println(ctx.Value(key), \"监控退出，停止了...\") return default: //取出值 fmt.Println(ctx.Value(key), \"goroutine监控中...\") time.Sleep(2 * time.Second) &#125; &#125;&#125; Context的使用注意事项 Context 始终要以参数的形式进行传递，整个结构体是不太行的，当然，利用map存储进行设计管理还是可以的。 Context 做参数你得永远把它放第一位，无论什么情况都切记 Context 可以在多个goroutine中进行传递，无需担心，它是线程安全的。 结尾两天终于把这个系列整完了，其实相当于自己复习了一下，也借鉴了一些别人的代码，value那个转手是在太多，我找不着原作者了，要是谁看到说是你写的，马上邮件我啊，我加上你的名字。 下一步我要开个大坑，我要用Golang 整个大活儿，爬虫之类的我都不想写了，我现在打算用Golang复写我的一些安全工具，或者是写一个Elasticsearch相关的东西，期待吧！","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang中WaitGroup使用的一点随想","slug":"Golang中WaitGroup使用的一点随想","date":"2020-05-26T05:29:11.000Z","updated":"2020-06-01T02:17:04.495Z","comments":true,"path":"2020/05/26/Golang中WaitGroup使用的一点随想/","link":"","permalink":"https://yemilice.com/2020/05/26/Golang%E4%B8%ADWaitGroup%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E7%82%B9%E9%9A%8F%E6%83%B3/","excerpt":"","text":"Golang中WaitGroup使用的一点随想前言（为什么又要写一篇随想文）上次我写了一个channel的文章，我寻思，这Golang控制三大巨头，channel，waitgroup，context，我得尽快都安排上，最近工作太忙，压力过大，但是Update Blog还是不能够停下来，所以继续补上，学习还是不能停，那么来吧。 WaitGroup的简单用法(等待组)你品一下人家这名字，等待组。等待什么，等待goroutine完成啊。有些时候，我们启动多个goroutine去执行任务，我举个例子 123456listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125;for _, ip := range(listip) &#123; //假设我们执行一个ping ip 的逻辑 go PingIPWork(ip)&#125; 我这里执行了一个多ip去ping的逻辑，一般这种时候，你要是执行一波，人家肯定毛都不会返回给你，为什么？因为人家主线程直接就退出了，还是那句话，你又没告诉人家主线程要等这ip全部都ping 完，所以你必须要加个等待，等着Goroutine完成，这里我再举一个网上的例子 123456789101112131415161718package mainimport ( \"fmt\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;() //来个睡眠，等Goroutine结束 time.Sleep(time.Second * 1)&#125; 看到了么，加了一个sleep，用sleep去等着Goroutine跑完，上面我举的那个例子也可以这么来 12345678listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125;for _, ip := range(listip) &#123; //假设我们执行一个ping ip 的逻辑 go PingIPWork(ip)&#125;time.Sleep(time.Second * 1) 加个sleep可以等待完成，但是万一啊，Goroutine有的跑的快，有的慢，你那sleep就一秒，要是有的Goroutine没跑完不就白瞎了吗，所以咱们需要一个机制，这个机制可以帮助咱们去管理Goroutine，让我们知道Goroutine这东西什么时候停，什么时候完成。所以，WaitGroup这个东西，就可以帮助我们解决这个问题，还是老样子，我举一个简单的例子来说明我的想法。 1234567891011121314151617181920212223242526272829303132333435package mainimport ( \"fmt\" \"sync\")func PingIPWork(ip string) &#123; fmt.Println(ip)&#125;func main() &#123; //定义一个等待阿祖 var wg sync.WaitGroup wg.Add(3) // 因为有3个Ip，咱们定义三个动作，所以来三个计数 listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125; for _, ip := range(listip) &#123; //假设我们执行一个ping ip 的逻辑 go func(ip string) &#123; //执行一个work PingIPWork(ip) //操作完成之后，done一个计数，也就是3-1 wg.Done() &#125;(ip) &#125; //等待 wg.Wait() // 等待，直到计数为0&#125; 这里我举了一个简单的例子，其实wg的用法较为简单，在这个例子里面我们用到了 12345678wg.wait等待Goroutine结束之后退出主进程wg.Add添加Goroutine，其实你可以把它想成，可添加的最大Goroutine数wg.Done想象成销毁参数，当Goroutine结束之后调用，意思就是，你没了，我减1 WaitGroup的其他注意事项将Wg作为参数进行传递的时候，需要使用指针有些时候，咱们不想写的这么麻烦，就寻思怎么才能简单一点，或者可变性稍微强一点，有些时候我们要把wg最为参数，在函数内部调用，我们该怎么写呢？ 123456789101112131415161718192021222324252627282930package mainimport ( \"fmt\" \"sync\")func PingIPWork(ip string, wg *sync.WaitGroup) &#123; fmt.Println(ip) wg.Done()&#125;func main() &#123; var wg sync.WaitGroup wg.Add(3) // 因为有两个动作，所以增加2个计数 listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125; for _, ip := range(listip) &#123; //假设我们执行一个ping ip 的逻辑 go PingIPWork(ip, &amp;wg) &#125; wg.Wait() // 等待，直到计数为0&#125; 看到了么，如果你把Wg作为参数进行传递，你得要用指针的形式传值，否则就会死锁！！！！！！！！ Wg.Add的数值不能为负wg.Add()的数值必须为正数，如果为负数，将会抛出异常。 1234567panic: sync: negative WaitGroup countergoroutine 1 [running]:sync.(*WaitGroup).Add(0xc042008230, 0xffffffffffffff9c) D:/Go/src/sync/waitgroup.go:75 +0x1d0main.main() D:/code/go/src/test-src/2-Package/sync/waitgroup/main.go:10 +0x54","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang中Channel使用的一点随想","slug":"Golang中Channel使用的一点随想","date":"2020-05-25T10:25:38.000Z","updated":"2020-05-25T11:37:10.053Z","comments":true,"path":"2020/05/25/Golang中Channel使用的一点随想/","link":"","permalink":"https://yemilice.com/2020/05/25/Golang%E4%B8%ADChannel%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E7%82%B9%E9%9A%8F%E6%83%B3/","excerpt":"","text":"Golang中Channel使用的一点随想前言（为什么要写这篇文章）在Golang中，搞同步/并发控制的方法有很多，有channel(管道)，WaitGroup(等待线程结束)，context(上下文管理)，我一直想深入研究一下它们，因为这次开发我遇到了很多比较棘手的问题，我认为万变不离其宗，所以我看了一下他们的源码，然后简单的写了几个Demo，结合了我自己的开发经验，写成此文，做记录的同时，希望可以帮到其他兄弟，未来我还会出context随想，waitgroup随想，一点一点来吧。 什么是channel首先你要了解两个东西，一个是goroutine，一个是CSP(Communicating Sequential Processes) goroutine：Go协程，比线程小，内存占用小，Go的主打 CSP:一种模型，并发模型，说白了，就是依赖channel，认为信息的载体更加重要。这里相对来说比较复杂，请大家参考 http://www.usingcsp.com/cspbook.pdf 去学习一个。 那么channel到底是什么呢，其实就是一种在goroutine之间通用的通信方式，这么理解吧，军队每天都看到人守夜，每天都有不同的口号，比如哨兵a，哨兵b进行交接的时候，就要对暗号，我们理解一下，哨兵a,b是两个goroutine，那么口令就是channel，通过channel，我们可以控制哨兵下岗，上岗，巡逻，那么换算到goroutine中，我们就可以控制goroutine启动，停止，这就是channel的牛逼之处。 channel长什么样？（我们怎么定义channel）channel的定义方法非常简单，利用chan 关键字就可以 1test := make(chan bool) 这里的make，没有定义缓存值，channel可以定义一个缓存值，例如 1make(chan int, 100) 这里代表channel 的缓存大小为100，如果不设置缓存值，那么channel没有缓存，在没有通讯的情况下，将被阻塞。 是的，这个就定义好了，现在你就有一个名叫test的bool类型的channel，你可能会问，卧槽，这东西有什么用？行，我马上就举个例子告诉你这玩意儿怎么使。我知道你们大部分都不爱看理论，只爱看解决问题的模型，没问题，我惯你！ 来个channel的并发例子你细分析分析，一般你什么时候会用到goroutine间通信？那不就是一个不够使，多开几个，增加咱们program的效率嘛。是，你可以用 go example() 这种类型，开它十几二十个，但是你怎么知道他们结束了呢？估摸着你写sleep啊，就像 123456listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125;for _, ip := range(listip) &#123; //假设我们执行一个ping ip 的逻辑 go PingIPWork(ip)&#125;time.sleep(time.Second * 5) 这样其实没啥毛病，因为你不加那个sleep，人家估摸着就会一闪而过，你什么消息都收不到，我们这里有三个Ip，相当于你 go PingIPWork(ip)这里执行，外部的主程会直接退出去，人家才不管你搞完没呢，你又没和人家说是吧，所以channel就是干这个的，就是告诉主程，里面还有人呢嘿，别关门！ 123456789101112listip := []string&#123;\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"&#125;ch := make(chan struct&#123;&#125;)for _, ip := range listip &#123; go func(ip string) &#123; //假设执行一个ping ip的逻辑 go PingIPWork(ip) ch &lt;- struct&#123;&#125;&#123;&#125; &#125;(ip)&#125;for range ips &#123; &lt;-ch&#125; 这里去并发去执行PingIPWork，并且主程会等待所有goroutine完成之后才会彻底退出。发现了没，channel和goroutine一般都是放在一起的。 在for…i range中使用channel有时候我们需要阻塞range，或者是控制循环不退出，这时候就可以用到channel 12345678910111213listip := [\"10.0.9.11\",\"10.0.9.22\",\"10.0.9.33\"]//创建channelforever := make(chan bool)go func() &#123; for _, d := range msgs &#123; log.Printf(\"Received a message: %s\", d) &#125;&#125;()//阻塞，让它不退出&lt;-forever 在select中使用channel类似switch的骚操作，一般来说，你给人家整死循环了，你不得负责给人家搞退出，或者说满足必要条件退出 1234567891011121314151617// 创建 quit channelquit := make(chan string)// 启动生产者 goroutinec := boring(\"Joe\", quit)// 从生产者 channel 读取结果for i := rand.Intn(10); i &gt;= 0; i-- &#123; fmt.Println(&lt;-c) &#125;// 通过 quit channel 通知生产者停止生产quit &lt;- \"Bye!\"fmt.Printf(\"Joe says: %q\\n\", &lt;-quit)select &#123;case c &lt;- fmt.Sprintf(\"%s: %d\", msg, i): fmt.Println(\"work....\")case &lt;-quit: quit &lt;- \"See you!\" return&#125; 有些时候我们不想等那么久，所以我们需要（timeout）机制 1234567891011c1 := make(chan string, 1)go func() &#123; time.Sleep(time.Second * 2) c1 &lt;- \"result 1\"&#125;()select &#123;case res := &lt;-c1: fmt.Println(res)case &lt;-time.After(time.Second * 1): fmt.Println(\"timeout 1\")&#125; 关闭一个channel不用的东西要打包放好，否则你开一大堆channel在那里，搞一大堆panic: send on closed channel很吼么？ 随意关闭channel的姿势你也要学到，其实关闭的逻辑也很简单，关键字close 123c := make(chan int, 10)c &lt;- 1close(c) 这就关掉了，但是这样关不严谨，你还是能拿到c已经发出去的数据，而且还能不断的读到0值，所以一定要换个方法 1234c := make(chan int, 10)close(c)i, ok := &lt;-cfmt.Printf(\"%d, %t\", i, ok) 这样就能判断，当close时，读到的值是零值还是正常值，也就避免了上面出现的那种情况，一直读，一直有。 后记我感觉我学的还是不够深，我希望我的技术能更进一步，所以我还是要不断学习，我可以的，我会做到的。这里面其实都是些皮毛。。我感觉，我希望未来一定要多学多读多看。哈哈，这篇文章写完了我也要去吃饭了，今天我吃仔肺粉 + 锅盔，我去吃饭啦！有问题给我留言或者邮件吧！ 写于2020-05-25 19:36分","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"分布式数据库如何选择，几种分布式数据库优缺点一览","slug":"分布式数据库如何选择，几种分布式数据库优缺点一览","date":"2020-05-22T08:16:20.000Z","updated":"2020-05-22T08:29:46.945Z","comments":true,"path":"2020/05/22/分布式数据库如何选择，几种分布式数据库优缺点一览/","link":"","permalink":"https://yemilice.com/2020/05/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%8C%E5%87%A0%E7%A7%8D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E7%BC%BA%E7%82%B9%E4%B8%80%E8%A7%88/","excerpt":"","text":"分布式数据库如何选择？几种分布式数据库优缺点一览为什么选择分布式数据库？优点如下： 具有灵活的体系结构 适应分布式的管理和控制机构 经济性能优越 系统的可靠性高、可用性好 局部应用的响应速度快 可扩展性好，易于集成现有系统。 相关的技术概念介绍什么是分布式数据库？常见的分布式系统分为， 支持持久化存储的分布式存储系统 着重计算的分布式计算框架 分布式消息队列 根据不同的应用的领域，把上述分类细化，常见分布式存储系统分为： 分布式协同系统（分布式日志复制） 分布式任务调度框架 流计算框架 分布式文件/对象系统 分布式NoSQL存储 分布式关系数据库（OLAP、OLTP）； 各种消息队列mq 不同的分布式数据库如何区分？ Key-value NoSQL : 例如Redis Riak等； column family NoSQL(wide column store) : 典型的是Hbase Cassandra document NoSQL : 典型的是mongodb 需要什么样的数据库? 支持数据持久化，数据落盘，异常备份，高并发，大数据量存储。 要支持频繁的数据读写 分布式，多节点并行 和以前的数据库不冲突 可选的方法及其特点 根据上述的要求，分布式数据库，符合大数据存储的，支持频繁读写的数据库有如下几个，它们的特点会简单说明。 Elasticsearch数据库Elasticsearch简介分布式的实时文件存储，每个字段都被索引并可被搜索，分布式的实时分析搜索引擎可以扩展到上百台服务器，处理PB级结构化或非结构化数据 Elasticsearch应用场景分布式的搜索引擎和数据分析引擎，全文检索，结构化检索，数据分析 对海量数据进行近实时的处理，站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析 Elasticsearch的优缺点缺点：没有用户验证和权限控制，没有事务的概念，不支持回滚，误删不能恢复，需要java环境. 优点：将你的文档分割到不同容器或者分片中，可以存在单个节点或多个节点复制每个分片提供数据备份，防止硬件问题导致数据丢失。 Elasticsearch的持久化方案gateway 代表 elasticsearch 索引的持久化存储方式，elasticsearch 默认是先把索引存放到内存中去，当内存满了的时候再持久化到硬盘里。当这个 elasticsearch 集群关闭或者再次重新启动时就会从 gateway 中读取索引数据。elasticsearch 支持多种类型的 gateway，有本地文件系统（默认），分布式文件系统，Hadoop 的 HDFS 和 amazon 的 s3 云存储服务。 ElasticSearch是先把索引的内容保存到内存之中，当内存不够时再把索引持久化到硬盘中，同时它还有一个队列，是在系统空闲时自动把索引写到硬盘中。 Redis数据库Redis简介redis是开源BSD许可高级的key-value存储系统(NoSQL)，可以用来存储字符串，哈希结构，链表，集合，因此，常用来提供数据结构服务，Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加 载进行使用。 支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。Redis支持数据的备份，即master-slave模式的数据备份。 Redis应用场景A）常规计数：粉丝数，微博数 B）用户信息变更 C）缓存处理，作为mysql的缓存 D）队列系统，建有优先级的队列系统，日志收集系统 Redis的优缺点优点(1) 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) (2) 支持丰富数据类型，支持string，list，set，sorted set，hash (3) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 (4) 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 缺点（1）Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复 （2）主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性 （3）redis的主从复制采用全量复制，复制过程中主机会fork出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦 （4）Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 Redis的持久化方案redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。 RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 Mongodb数据库Mongodb简介MongoDB本身是一种非关系型数据库。它的每一条记录是一个Document，每个Document有一组键值对组成。MongoDB中的Document与JSON对象相似。 Document中字段的值可能包括其他Document，数组等。 Mongodb应用场景mongodb的主要目标是在键/值存储方式（提供了高性能和高度伸缩性）以及传统的RDBMS系统（丰富的功能）架起一座桥梁，集两者的优势于一身。mongo适用于以下场景： a.网站数据：mongo非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 b.缓存：由于性能很高，mongo也适合作为信息基础设施的缓存层。在系统重启之后，由mongo搭建的持久化缓存可以避免下层的数据源过载。 c.大尺寸、低价值的数据：使用传统的关系数据库存储一些数据时可能会比较贵，在此之前，很多程序员往往会选择传统的文件进行存储。 d.高伸缩性的场景：mongo非常适合由数十或者数百台服务器组成的数据库。 e.用于对象及JSON数据的存储：mongo的BSON数据格式非常适合文档格式化的存储及查询。 Mongodb的优缺点优点： (1) 弱一致性（最终一致），更能保证用户的访问速度 (2) 文档结构的存储方式，能够更便捷的获取数据 (3) 内置GridFS，支持大容量的存储 (4) 在使用场合下，千万级别的文档对象，近10G的数据，对有索引的ID的查询不会比mysql慢，而对非索引字段的查询，则是全面胜出。 缺点： （1）不支持事物 （2）占用空间过大，会造成磁盘浪费 （3）单机可靠性比较差 （4）大数据量持续插入，写入性能有较大波动 Mongodb的持久化方案/异常处理当执行写操作时，MongoDB创建一个journal来包含确切磁盘位置和改变的字节。因此，如果服务器突然崩溃，启动时，journal会重放崩溃前并没有刷新到磁盘上的任何写操作。 数据文件每隔60s刷新到磁盘上，默认情况下，因此journal只需要持有60s内的写入数据。journal预分配了几个空文件用于此目的，位于/data/db/journal，命名为_j.0,j.1等等。 MongoDB运行很长时间情况下，在journal目录下，你会看到类似于_j.6217,_j.6218和_j.6219文件。这些文件是当前的journal文件，如果MongoDB一直运行，这些数字会持续增加。当正常关闭MongoDB时，这些文件将被清除，因为正常关机不在需要这些日志的。 如果服务器崩溃或kill -9, mongodb再次启动时，会重放journal文件，会输出冗长难懂的检验行，这表明在正常的恢复。 Mysql分布式集群Mysql分布式集群简介MySQL集群是一个无共享的(shared-nothing)、分布式节点架构的存储方案，其目的是提供容错性和高性能。 数据更新使用读已提交隔离级别（read-committedisolation)来保证所有节点数据的一致性，使用两阶段提交机制（two-phasedcommit)保证所有节点都有相同的数据(如果任何一个写操作失败，则更新失败）。 无共享的对等节点使得某台服务器上的更新操作在其他服务器上立即可见。传播更新使用一种复杂的通信机制，这一机制专用来提供跨网络的高吞吐量。 通过多个MySQL服务器分配负载，从而最大程序地达到高性能，通过在不同位置存储数据保证高可用性和冗余。 Mysql分布式集群应用场景解决海量存储问题，比如京东B2B就用的Mysql分布式集群。 适用几十亿的PV对DB的访问。 Mysql分布式集群的优缺点优点： a) 高可用性 b)快速的自动失效切换 c)灵活的分布式体系结构，没有单点故障 d)高吞吐量和低延迟 e)可扩展性强，支持在线扩容 缺点： a)存在很多限制，比如：不支持外键 b)部署、管理、配置很复杂 c)占用磁盘空间大，内存大 d)备份和恢复不方便 e)重启的时候，数据节点将数据load到内存需要很长时间 Mysql分布式集群的持久化方案负载均衡。 管理节点备份。","categories":[],"tags":[{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]},{"title":"开发中常用的Golang高级用法","slug":"开发中常用的Golang高级用法","date":"2020-05-22T06:37:09.000Z","updated":"2020-05-22T08:04:26.123Z","comments":true,"path":"2020/05/22/开发中常用的Golang高级用法/","link":"","permalink":"https://yemilice.com/2020/05/22/%E5%BC%80%E5%8F%91%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Golang%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/","excerpt":"","text":"开发中常用的Golang高级用法前言忙碌了两个月，这次开发终于要结束了，今天下午公司在重组集群机器，也没办法干活儿了，就写一些东西，相当于，留住一些东西，来纪念这辛苦的两个月吧。做一个纪念，也是为了方便以后自己去查看。在这次开发中，学习了不少Golang的高级特性，并且付诸于实现，也踩了不少坑，留下这篇文字，也是方便其他人能够查看，或者借鉴，如果帮到你，那么我也会很开心你。 开发常遇到的问题Golang判断一个元素在不在切片/列表当中在Python中，我们可以直接用 in 的方式去判断，例如 1if \"i\" in lists 但是在Golang中，没有这种语法糖或者是关键字可以帮助我们处理这种问题，所以还是只能靠循环去处理这种问题，为此我封装了一个Golang函数，函数如下 123456789//FindType 循环对比，匹配到返回true，不匹配返回falsefunc FindType(a string, typelist []string) bool &#123; for _, b := range typelist &#123; if b == a &#123; return true &#125; &#125; return false&#125; Golang获取文件的详细信息Golang获取文件信息的方法相对来说容易一些，都已经有了对应的package，我这里只是把怎么用展示出来 12345678910111213141516171819202122232425262728//timespecToTime 转换func timespecToTime(ts syscall.Timespec) time.Time &#123; return time.Unix(int64(ts.Sec), int64(ts.Nsec))&#125;//GetFileinfo 获取文件信息func GetFileinfo(path string) &#123; fileInfo, err := os.Stat(path) if err != nil &#123; return 0 &#125; //文件大小 filesize := fileInfo.Size() //文件创建时间 stat_ts := fileInfo.Sys().(*syscall.Stat_t) Ctime := timespecToTime(stat_ts.Ctim).Format(\"2006/01/02\") //文件修改时间 Mtime := timespecToTime(stat_ts.Mtim).Format(\"2006/01/02\") //文件访问时间 Attim := timespecToTime(stat_ts.Atim).Format(\"2006/01/02\") //获取文件所有者 stat_ts := fileInfo.Sys().(*syscall.Stat_t) uid := strconv.Itoa(int(stat_ts.Uid)) usrs, err := user.LookupId(string(uid)) username := usrs.Username //获取文件名 filename := fileInfo.Name()&#125; Golang比较两个list/切片的不同之处（差集）在开发中，我需要比较两个list，然后取出他们之中不同的部分，这里Golang也没有合适的法子，一般的方法就是转map进行处理 12345678910111213141516171819202122232425262728293031323334353637383940//difference 进行比对，输出不同的[]stringfunc difference(slice1, slice2 []string) []string &#123; m := make(map[string]int) nn := make([]string, 0) inter := intersect(slice1, slice2) for _, v := range inter &#123; m[v]++ &#125; for _, value := range slice1 &#123; times, _ := m[value] if times == 0 &#123; nn = append(nn, value) &#125; &#125; return nn&#125;//intersect 把两个对比的列表进行map化处理func intersect(slice1, slice2 []string) []string &#123; m := make(map[string]int) nn := make([]string, 0) for _, v := range slice1 &#123; m[v]++ &#125; for _, v := range slice2 &#123; times, _ := m[v] if times == 1 &#123; nn = append(nn, v) &#125; &#125; return nn&#125;func main() &#123; infinode := []string&#123;\"10.0.9.1\",\"10.0.9.2\"&#125; syncnodes := []string&#123;\"10.0.9.1\",\"10.0.9.2\", \"10.0.9.3\"&#125; ips := difference(infinode, syncnodes)&#125; Golang格式化时间golang 一般都可以通过Format的方法进行时间格式化，一般你可以直接调format，例如 1fmt.Println(time.Now().Format(\"2006-01-02 15:04:05\")) 有些时候是他娘的时间戳 1fmt.Println(time.Unix(1389028339, 0).Format(\"2006-01-02 15:04:05\")) 有些时候会让你搞成别的样子，比如”2006/01/02 15:04:05” 1fmt.Println(time.Now().Format(\"2006/01/02 15:04:05\")) 有些时候会让你比较个时间大小，例如 123456789101112//比对时间，看看它在不在开始时间/结束时间的范围内starttime := \"2020-05-12\"endtime := \"2020-05-20\"ctime := \"2020-05-15\"ft, err := time.Parse(\"2006-01-02\", ctime)st, err := time.Parse(\"2006-01-02\", starttime)et, err := time.Parse(\"2006-01-02\", endtime)if ft.After(et) &amp;&amp; ft.Before(st) &#123; fmt.Println(\"在里面儿！\")&#125; else &#123; fmt.Println(\"不在里面儿！\")&#125; Golang并发循环的使用我们处理循环的时候，在Python中，我举个例子 123lists = [1, 2, 3, 4]for i in lists: print i 这里打印的话是 12341234 Golang有个黑科技是并发循环，简单说，就是能一下给你把这四个都给你打印出来，不是一条条循环，也不用等上一个循环的结果返回，也可以选择略过错误。我在这里简单实现一个 直接并发循环（无需考虑错误）这里不考虑error的情况，具体代码如下 12345678910111213ips := []string&#123;\"10.0.9.1\",\"10.0.9.2\"&#125;ch := make(chan struct&#123;&#125;)//并发循环ipfor _, ip := range ips &#123; go func(ip string) &#123; //模拟一个展示ip fmt.Println(ip) ch &lt;- struct&#123;&#125;&#123;&#125; &#125;(ip)&#125;for range ips &#123; &lt;-ch&#125; 输出错误的并发循环(考虑错误)1234567891011121314ips := []string&#123;\"10.0.9.1\",\"10.0.9.2\"&#125;//定义一个errorerrors := make(chan error)for _, ip := range ips &#123; go func(ip string) &#123; _, err := test(ip) errors &lt;- err &#125;(ip)&#125;for range ips &#123; if err := &lt;-errors; err != nil &#123; return err &#125;&#125; Golang做一个不退出的无限循环有些时候我们希望一个服务/脚本/函数不断运行，或者每隔一段时间来一发（运行一次），这时候我们就需要定义无限循环 1234567func main() &#123; for &#123; //每隔1s打印一次start work time.Sleep(time.Second * 1) fmt.Println(\"Start work......\") &#125;&#125; Golang实现一个遇到错误的重试机制当我们最开发的时候，有些时候遇到错误，需要进行重试，这时候我们就需要进行错误捕获和函数重载 1234567891011121314151617181920212223//Retry 重试逻辑//传入函数，如果捕获到错误，则重载函数，重新来一次执行，直到没错误为止func Retry(fn func() error) error &#123; if err := fn(); err != nil &#123; if s, ok := err.(stop); ok &#123; return s.error &#125; return Retry(fn) &#125; return nil&#125;//TestWork 测试函数，无意义func TestWork() error &#123; //测试函数，这里是伪代码。 _, err := GetIpNode() if err != nil &#123; return err &#125; return nil&#125;func main() &#123; go Retry(TestWork)&#125; Golang执行Linux命令有些时候我们需要执行linux相关命令，Golang封装了相应的包，但是少部分时候我们需要控制一些很久不返回结果的命令，所以我加了一个超时时间，代码如下 1234567891011121314151617181920212223242526272829303132var ( //这里我写死了，你可以自己定义，更灵活一些 Timeout = 60 * time.Second)//Command 执行shellfunc Command(arg string) ([]byte, error) &#123; ctxt, cancel := context.WithTimeout(context.Background(), Timeout) defer cancel() cmd := exec.CommandContext(ctxt, \"/bin/bash\", \"-c\", arg) var buf bytes.Buffer cmd.Stdout = &amp;buf cmd.Stderr = &amp;buf if err := cmd.Start(); err != nil &#123; return buf.Bytes(), err &#125; if err := cmd.Wait(); err != nil &#123; return buf.Bytes(), err &#125; return buf.Bytes(), nil&#125;func main() &#123; _, err := Command(\"ls\") if err != nil &#123; fmt.Println(err) &#125;&#125; Golang 简单的日志逻辑这边贡献一个简单的日志实现代码，往文件里面写，可以自己定义格式，代码如下 1234567891011121314151617181920212223242526272829303132333435import ( \"fmt\" \"os\" \"github.com/op/go-logging\")func Monlog() (logs *logging.Logger) &#123; var log = logging.MustGetLogger(\"monlog\") //定义格式 时间 go文件 行数 等级 错误信息 var format = logging.MustStringFormatter( `%&#123;time:2006-01-02T15:04:05&#125; %&#123;shortfile&#125; %&#123;shortfunc&#125; %&#123;level&#125; %&#123;message&#125;`) //文件的写入位置 追加模式/没有自动创建 logFile, err := os.OpenFile(\"/var/log/monlog.log\", os.O_WRONLY|os.O_APPEND|os.O_CREATE, 0666) if err != nil &#123; panic(fmt.Sprintf(\"Open faile error!\")) &#125; //头相关, 具体意思就是支持追加，然后格式化，且按照那个格式往里面写 backend1 := logging.NewLogBackend(logFile, \"\", 0) backend2 := logging.NewLogBackend(os.Stderr, \"\", 0) backend2Formatter := logging.NewBackendFormatter(backend2, format) backend1Formatter := logging.NewBackendFormatter(backend1, format) backend1Leveled := logging.AddModuleLevel(backend1Formatter) backend1Leveled.SetLevel(logging.INFO, \"\") logging.SetBackend(backend1Leveled, backend2Formatter) return log&#125;func main() &#123; log := Monlog() log.Info(\"info\") log.Notice(\"notice\") log.Warning(\"warning\")&#125; Golang 使用 Aws-sdk 获取到指定bucket中全部的item我前阵子写过一篇文章，是Golang 调用 s3对象存储的，使用指定的api可以获取其中的item，但是人家限定100条，估摸着怕撑爆内存，但是如果我们想要获取到所有的item，这个该怎么做呢？其实循环获取就好了，但是我还是徒手实现一下吧，大家抄抄得了。 1234567891011121314151617181920212223242526func GetRgwItem3(buckets string) ([]*s3.Object, error) &#123; var items []*s3.Object sess, err := session.NewSession(&amp;aws.Config&#123; Credentials: credentials.NewStaticCredentials(ak, sk, \"\"), Endpoint: aws.String(endpoint + \":7480\"), Region: aws.String(\"us-east-1\"), DisableSSL: aws.Bool(true), S3ForcePathStyle: aws.Bool(false), //virtual-host style方式，不要修改 &#125;) if err != nil &#123; return nil, err &#125; svc := s3.New(sess) err := svc.ListObjectsPages(&amp;s3.ListObjectsInput&#123; Bucket: &amp;buckets, &#125;, func(p *s3.ListObjectsOutput, last bool) (shouldContinue bool) &#123; for _, obj := range p.Contents &#123; items = append(items, obj) &#125; return false &#125;) if err != nil &#123; return items, err &#125; return items, nil&#125; 我这里没毛病啊，万一你有问题你就咨询我 后记暂时先更这么多，这也不少了，大部分代码我都给出了具体实现，要是还不会就留言给我或者发邮件。 其实我特想对那些写几个爬虫在那里爬我博客的人说，你偷博客没所谓，复制也没所谓，不加名字说转载就很说不过去了，还TM标榜你是原创，脸呢？cnblog我会逐渐弃掉，因为我有我自己的博客了，我也在逐渐搞迁移，把我原来写的博客都迁移到我自己的博客上去，未来我的cnblog博客将逐渐少更或者停更，就算更我也会用英语/日语双更，我让你TM抄我东西不说，fuck off bitch。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang封装Elasticsearch常用功能","slug":"Golang封装Elasticsearch常用功能","date":"2020-05-14T01:45:49.000Z","updated":"2020-05-14T01:50:26.323Z","comments":true,"path":"2020/05/14/Golang封装Elasticsearch常用功能/","link":"","permalink":"https://yemilice.com/2020/05/14/Golang%E5%B0%81%E8%A3%85Elasticsearch%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD/","excerpt":"","text":"前言（为什么要写这篇文章）首先看过我博客的都应该知道，我去年发了一篇Python封装Elasticsearch的文章。但那是去年了，今年我将我的检索服务后端用Golang全部重写了一波，相当于用Go重构了以前的Python代码，不过我个人感觉Golang的效率还是高于Python的，而且我还加了一些异常判断和处理，这次的代码只会比以前更好更牛逼，为了纪念这一个多月的重构历程，我将关键功能记录下来，方便自己复习和各位兄弟姐妹查看。 使用的Go包我的Elasticsearch版本是6.3.2，6系列了，现在（2020-05-13）最新版本应该是7，不过新版本和旧版本应该就是少了Type，我的6版本代码，请各位自己自行斟酌使用。安装指定的Go包 olivere/elastic，现在有官方驱动的包了，但是我这篇文章用的包是 olivere/elastic，所以一切的代码都是以olivere为主。 1go get github.com/olivere/elastic 基础的使用（Simple的使用）接下来我举几个例子来说下这个golang 如何驱动 elasticsearch的 连接Elasticserach1234567891011121314151617181920212223package elasticdbimport ( \"context\" \"fmt\" \"github.com/olivere/elastic\")func main() &#123; //连接127.0.0.1 client, err := elastic.NewClient(elastic.SetURL(\"http://127.0.0.1:9200\")) if err != nil &#123; fmt.Println(err) return &#125; //检查健康的状况，ping指定ip，不通报错 _, _, err = client.Ping(ip).Do(context.Background()) if err != nil &#123; fmt.Println(err) return &#125;&#125; 创建一个index索引这里我默认你们都有一定的Es基础，其实你把index想成Mysql里面的表就可以了。 123456789//indexname 你可以想成表名func CreateIndex(indexname string) &#123; client, err := elastic.NewClient(elastic.SetURL(\"http://127.0.0.1:9200\")) if err != nil &#123; fmt.Println(err) return &#125; client.CreateIndex(indexname).Do(context.Background())&#125; 删除一个index索引想成删除一张表 123456789//indexname 你可以想成表名func CreateIndex(indexname string) &#123; client, err := elastic.NewClient(elastic.SetURL(\"http://127.0.0.1:9200\")) if err != nil &#123; fmt.Println(err) return &#125; client.DeleteIndex(indexname).Do(context.Background())&#125; 往指定的index当中导一条数据想成往一张表里面导入一条数据，在Golang中，我们可以导入json的字符串，我们也可以导入golang的struct类型，例如 1234567//结构体type Task struct &#123; Taskid string `json:\"taskid\"` Taskname string `json:\"taskname\"`&#125;//字符串jsonmsg := `&#123;\"taskid\":\"123456\", \"taskname\":\"lwb\"&#125;` 1234567891011121314151617181920212223//导入数据，你需要index名，index的type，导入的数据func PutData(index string, typ string, bodyJSON interface&#123;&#125;) bool &#123; client, _ := elastic.NewClient(elastic.SetURL(\"http://127.0.0.1:9200\")) _, err := client.Index(). Index(index). Type(typ). BodyJson(bodyJSON). Do(context.Background()) if err != nil &#123; //验证是否导入成功 fmt.Sprintf(\"&lt;Put&gt; some error occurred when put. err:%s\", err.Error()) return false &#125; return true&#125;func main() &#123; //json字符串导入 jsonmsg = `&#123;\"taskid\":\"123456\", \"taskname\":\"hahah\"&#125;` status := PutData(\"test\", \"doc\", jsonmsg) //struct结构体导入 task := Task&#123;Taskid: \"123\", Taskname: \"hahah\"&#125; status := PutData(\"test\", \"doc\", task)&#125; 删除一条数据删除数据需要ID，这个ID是个啥玩意儿呢。。。就是咱们不是刚导了一条数据进去么，你可以设置这数据的唯一ID，也可以让Elasticsearch帮你自动生成一个，一般没事儿干谁自己设置啊，还容易重复，一重复就报错。。我在这里把这个删除的方法教给大家，记住这个ID一定是唯一的 12345678func DeleteData(index, typ, id string) &#123; client, _ := elastic.NewClient(elastic.SetURL(\"http://127.0.0.1:9200\")) _, err := client.Delete().Index(index).Type(typ).Id(id).Do(context.Background()) if err != nil &#123; fmt.Println(err) return &#125;&#125; 高阶使用（条件查询／封装等）简单讲了一下增删改，现在我们来讲一下高阶用法，高级增删改查吧，其实官方的文档讲的还算是比较清楚，不过我等大中华程序狗的姿势水平。。。至少我是图样图森破，看英文也算是废了九牛二虎之力，算是捋出来了一些高阶用法，顺手自己造了个轮子，现在我就来挑几点来说下吧。 自动选择可用的Es节点配合olivere的ping机制，可以做一个自动检测Es ip 是否可用的逻辑，这样可以增加我们put，updatge时候的稳定性 自动检测节点Elasticseach的IP是否可用olivere的Elasticserach sdk 限定了只能用一个ip，类似“http://127.0.0.1:9200”这样，我对原本的逻辑进行了一点改造，改成支持一个ip list，依次检测Es ip 是否可用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package elasticdbimport ( \"context\" \"fmt\" \"github.com/olivere/elastic\")//Elastic es的连接，type Elastic struct &#123; Client *elastic.Client host string&#125;//Connect 基础的连接代码func Connect(ip string) (*Elastic, error) &#123; //引入IP client, err := elastic.NewClient(elastic.SetURL(ip)) if err != nil &#123; return nil, err &#125; //Ping 的方式检查是否可用 _, _, err = client.Ping(ip).Do(context.Background()) if err != nil &#123; return nil, err &#125; //输出一个struct类型，可以被继承 es := &amp;Elastic&#123; Client: client, host: ip, &#125; return es, nil&#125;//InitES 初始化Es连接func InitES() (*Elastic, error) &#123; //host是一个列表 host := []string&#123;\"http://10.0.6.245:9200\",\"http://10.0.6.246:9200\",\"http://10.0.6.247:9200\"&#125; //统计host的数量 Eslistsnum := len(host) //如果为零就不继续接下来的逻辑 if Eslistsnum == 0 &#123; return nil, fmt.Errorf(\"Cluster Not Es Node\") &#125; //创建新的连接 for i, ip := range host &#123; //判断是不是最后一个节点ip if (Eslistsnum - 1) != i &#123; es, err := Connect(ip) //如果连接出错，则跳过 if err != nil &#123; fmt.Println(err) continue &#125; return es, nil //如果是最后一个节点 &#125; else &#123; es, err := Connect(ip) //输出错误 if err != nil &#123; return nil, err &#125; return es, nil &#125; &#125; return nil, nil&#125; 后续我们可以采用继承的方法调用Es的client的连接，这个在后面我就不详细说了，聪明的你，看代码一定能整明白，再整不明白，你就直接上Git拷贝我的代码就得了。 条件查询以前我写过一个Python的Elasticsearch Sdk，那里面的查询基本都用了query，简单来说，就是你，给Es的api发一个query，es给你返回一个查询结果。这里我会举几个常用的条件查询例子，然后用golang封装一波。这里我先定义一下数据结构，假设我们的Elasticsearch中，有一个叫做Task的index(索引)，其中存储着很多task的运行日志，它们的数据格式如下: 12345678&#123; \"taskid\": \"081c255b-936c-11ea-8001-000000000000\", \"starttime\": \"2020/05/13 18:38:21\", \"endtime\": \"2020/05/13 18:38:47\", \"name\": \"cifs01\", \"status\": 1, \"count\": 365&#125; 我们现在要做的就是围绕task这个index和其中的数据做条件查找的例子，我说的很明白了吧？开工了！ 查询时间范围/年龄大小的条件查询方法在业务需求中，我们经常会检索各种各样的数据，其中，范围查找应该是用的比较多的，所以我把它放到了最前面。 12345678910111213141516171819202122type Task struct &#123; TaskID string `json:\"taskid\"` StartTime string `json:\"starttime\"` EndTime string `json:\"endtime\"` Name string `json:\"name\"` Status int `json:\"status\"` Count int `json:\"count\"`&#125;//查找时间范围大于2020/05/13 18:38:21，并且小于2020/05/14 18:38:21的数据func (Es *Elastic) FindTime() &#123; var typ Task boolQ := elastic.NewBoolQuery() //生成查询语句，筛选starttime字段，查找大于2020/05/13 18:38:21，并且小于2020/05/14 18:38:21的数据 boolQ.Filter(elastic.NewRangeQuery(\"starttime\").Gte(\"2020/05/13 18:38:21\"), elastic.NewRangeQuery(\"starttime\").Lte(\"2020/05/14 18:38:21\")) res, _ := Es.Client.Search(\"task\").Type(\"doc\").Query(boolQ).Do(context.Background()) //从搜索结果中取数据的方法 for _, item := range res.Each(reflect.TypeOf(typ)) &#123; if t, ok := item.(Task); ok &#123; fmt.Println(t) &#125; &#125;&#125; 查询包含关键字的查询方法我们经常遇到那种，搜那么一两个字，让你展示所有包含这一两个字的结果，就像百度，你搜个”开发”，就能搜出来例如”软件开发”,”硬件开发”等。接下来咱们也实现一个这个功能 1234567891011121314//查找包含\"cifs\"的所有数据func (Es *Elastic) FindKeyword() &#123; //因为不确定cifs如何出现，可能是cifs01，也可能是01cifs，所以采用这种方法 keyword := \"cifs\" keys := fmt.Sprintf(\"name:*%s*\", keyword) boolQ.Filter(elastic.NewQueryStringQuery(keys)) res, _ := Es.Client.Search(\"task\").Type(\"doc\").Query(boolQ).Do(context.Background()) //从搜索结果中取数据的方法 for _, item := range res.Each(reflect.TypeOf(typ)) &#123; if t, ok := item.(Task); ok &#123; fmt.Println(t) &#125; &#125;&#125; 多条件查询如果说我们现在不仅仅需要找到符合时间的，也需要找到符合关键字的查询，那么就需要在查询条件上做文章。 12345678910111213func (Es *Elastic) FindAll() &#123; //因为不确定cifs如何出现，可能是cifs01，也可能是01cifs，所以采用这种方法 keyword := \"cifs\" keys := fmt.Sprintf(\"name:*%s*\", keyword) boolQ.Filter(elastic.NewRangeQuery(\"starttime\").Gte(\"2020/05/13 18:38:21\"), elastic.NewRangeQuery(\"starttime\").Lte(\"2020/05/14 18:38:21\"), elastic.NewQueryStringQuery(keys)) res, err := Es.Client.Search(\"task\").Type(\"doc\").Query(boolQ).Do(context.Background()) //从搜索结果中取数据的方法 for _, item := range res.Each(reflect.TypeOf(typ)) &#123; if t, ok := item.(Task); ok &#123; fmt.Println(t) &#125; &#125;&#125; 统计数量/多条件统计数量有些时候我们需要去统计符合查询条件的结果数量，做统计用，这里也有直接可用的Sdk 12345678910func (Es *Elastic) GetTaskLogCount() (int, error) &#123; boolQ := elastic.NewBoolQuery() boolQ.Filter(elastic.NewRangeQuery(\"starttime\").Gte(\"2020/05/13 18:38:21\"), elastic.NewRangeQuery(\"starttime\").Lte(\"2020/05/14 18:38:21\")) //统计count count, err := Es.Client.Count(\"task\").Type(\"doc\").Query(boolQ).Do(context.Background()) if err != nil &#123; return 0, nil &#125; return int(count), nil&#125; 总结我这边完成了几个查询/导入的基础功能，当然，我的代码大部分都放置在了github当中放置在: https://github.com/Alexanderklau/Go_poject/tree/master/Go-Elasticdb/Elasticsearch_sdk最近项目比较忙，我打算月中写一篇我开发的时候使用的一些Go特性，或者高级用法。如果喜欢的话麻烦Star我！最近压力颇大，想要换一个地方生活，所以也要准备离开了。如果大家有什么问题，可以直接给我提问，我看到了就会帮助大家的。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"这次绩效考评会成为压垮我的稻草么？","slug":"这次绩效考评会成为压垮我的稻草么？","date":"2020-04-14T02:06:30.000Z","updated":"2020-04-14T02:07:17.819Z","comments":true,"path":"2020/04/14/这次绩效考评会成为压垮我的稻草么？/","link":"","permalink":"https://yemilice.com/2020/04/14/%E8%BF%99%E6%AC%A1%E7%BB%A9%E6%95%88%E8%80%83%E8%AF%84%E4%BC%9A%E6%88%90%E4%B8%BA%E5%8E%8B%E5%9E%AE%E6%88%91%E7%9A%84%E7%A8%BB%E8%8D%89%E4%B9%88%EF%BC%9F/","excerpt":"","text":"写这篇文章的原因按理说，你们看过我博客的人，都知道，我是一个相当乐观的人，我从来都不会怎么丧气一些事儿，特别是在工作上，我永远都是顶在最前面，有困难bug我来，有难题我来攻关，按理说我就像是一个长者，身经百战，见的多了！那些naive的问题，我都不放在心上，可今天，恩，一个绩效考评，算是让我有了一些感到心累或者说是心寒吧。 发生了什么事儿说起来也是很尴尬，大家都知道，因为疫情，我整个二月和三月都在家办公，我那电脑没带回家，我用我家旧电脑和我舅舅的电脑，算是拼了个单片机，没图形界面那种，这是前提啊！本来我这边在开发一个比较大的项目，这项目，怎么说呢，用许嵩那句话就是 “作词作曲都是我自己”，咱不说别的，从策划，到架构涉及包括一些有的没的，都是我一个人搞定，那家伙，就差说我是CTO了哈哈，再一个，我作为整个项目组排头兵，第一个吃螃蟹涉足Golang开发，从一无所有，到文档齐备，就用了1月底，到2月底，这一个月时间，在家开发，日夜不眠不休，真正996，头发都给我掉了多少哈啊哈。这转折来了嘿，3月初，我有天正干活儿呢，你们知道，那开发机，一个人分好几台，我在我那几台开发机上正挥汗如雨呢，结果突然黑屏了，那一瞬间，我还傻x的拍了一下我电脑的后盖，我一寻思，不对啊，这不是电视，黑屏那就得是出事儿！果不其然，我把记录一发群里，才知道原来测试那边儿，有人在清理集群！一不小心把开发集群给清了！我勒个大擦！我寻思你这是灭霸啊，人灭霸打个响指，害得摆个造型。你这不声不响的，按个enter，就给我一个多月努力干没了，你可真是现代爆破专家啊你。这TM叫删库，你丫想跑路了吧！ 我当时的想法我当时脑子里先是一片空白，然后我脑子里一直弥漫着老八那句话，奥莉给！干他就完事儿了！是，我当时那个气啊，我就想给他丫干了，X的，小爷不说别的，以前也算是胡同口拍黑砖第一人啊！然后我的老大，紧急发来微信，问，你备份了吗？我说，没，那集群20多台机器呢，我在其中几台配了定时同步。我老大说，完了。我知道，我被删库了。 删库的损失不说别的，项目代码损失是最大的，2w行代码，一个月不眠不休的成果，完蛋了，等于这个月白干，是呗，是我倒霉么？人一倒霉，喝凉水都塞牙，给我气的。同时丢失的，还有Elasticsearch的work脚本，优化过后的ETCD封装代码，大部分文档，编译机的环境和包。 为什么不备份？首先开发集群有20多台机器，其中我的机器就用将近8台，8台机器啊老铁们，你会想到它们一瞬间都被干死么？好吧，我承认，我那个黑不溜秋的单片机linux只能处理一些逻辑问题，30G硬盘，安个Golang，python，jre都够呛了，害得跑个带gopls的Vim，备份？我这不寻思我过两天回去了备份么！ 绩效的定义写这个原因是，今儿（2020-4-13），考评下来，我老大找我谈话，说这次考评给了我C。也就是我绩效考核，3月份是C。我当时就疑惑了，删库的第一不是我，第二我不眠不休写的东西被人删了，我TM才是受害者！it‘s me！你给受害者考勤打C？我的项目都进测试部分了，你把开发机测试机一起干死了，当中是有我没备份的原因，但是你直接给我打C？？？我一打听，删库的人也是C，我勒个去，你这什么意思，删库的人和我都是C的评级？？那你是真的牛皮，感情我被删库我还成C了是吧！那你要追究责任，怎么不追究运维，为什么他会给删库的人那么大权限？让我背锅也太无耻了吧！ 结尾这次我感觉我有点寒心，是真的，我平常都写技术，很少写这种令人难过的玩意儿，但是我今天真的感觉心很累，也很寒心。看起来总是要人背锅，也就只有我了呗。呵呵。晚安了啊，如果哪位技术大佬或者是HR看到这篇文章，请不要误会，这只是我对这件事的一种寒心，我还是会继续输出高质量文章的。over","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://yemilice.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}],"keywords":[]},{"title":"Golang调用Rabbitmq消息队列和封装","slug":"Golang调用Rabbitmq消息队列和封装","date":"2020-04-13T02:11:00.000Z","updated":"2020-04-13T02:19:34.927Z","comments":true,"path":"2020/04/13/Golang调用Rabbitmq消息队列和封装/","link":"","permalink":"https://yemilice.com/2020/04/13/Golang%E8%B0%83%E7%94%A8Rabbitmq%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%92%8C%E5%B0%81%E8%A3%85/","excerpt":"","text":"前言介绍RabbimqRabbitmq消息队列是干嘛的？简单的说，消息队列，引申一下就是传递消息用的队列，也可以称为传递消息的通信方法。用争抢订单的快车举个例子，假如，A用户发送了一个用车的消息，那么消息队列要做的就是把A用户用车的这个消息广而告之，发送到一个公用队列当中，司机只管取到消息，而不管是谁发布的，这就是一个简单的消息队列例子，Rabbitmq其实就是消息队列的一种，用的比较多的还可能有Redis，kafka，ActiceMq等等，这个后面的博文里面我会说，这次我们只说Rabbimq消息队列 Rabbitmq消息队列的好处是什么？为什么我们要用他？这个网上有很多类似的玩意，我不说太多，就只说我在使用中感觉比较好的地方。 分布式，多节点部署。一个集群，保证消息的持久化和高可用，某节点挂了，其他节点可以结力。 路由Exchange，这个已经提供了内部的几种实现方法，可以指定路由，也就是指定传递的地址。 多语言支持，我以前干活儿用Python，现在用Go和java，人家无缝对接，多牛逼！ Ack的消息确认机制，这样就保证了，任务下发时候的稳定性，ack消息确认可以手动，也可以自动，这样就保证了任务下发时候的可控和监控。 初步开始简单的生产者和消费者的模型讲那么多废话理论，还不如直接开始写代码更直观是吧，所以，奥莉给，干了兄弟们！我们实现一个简答的生产者，消费者模型。这个不用我多解释吧，基础的流程就是，我们定义一个生产者，生产信息到Rabbitmq中，然后再定义一个消费者，把数据从Rabbitmq中取出来，就这么简单，下面咱们就干了，先讲几个基础。 Rabbitmq的基础知识发送 Publish发送，你可以理解为上传，意思就是，上传一个消息到Rabbitmq当中。它这块的基础代码比较简单 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( \"log\" \"github.com/streadway/amqp\")func main() &#123; //初始化一个Rabbimtq连接，后跟ip，user，password conn, err := amqp.Dial(\"amqp://guest:guest@localhost:5672/\") if err != nil &#123; return &#125; defer conn.Close() //创建一个channel的套接字连接 ch, _ := conn.Channel() //创建一个指定的队列 q, _ := ch.QueueDeclare( \"work\", // 队列名 false, // durable false, // 不使用删除？ false, // exclusive false, // 不必等待 nil, // arguments ) //定义上传的消息 body := \"work message\" //调用Publish上传消息1到指定的work队列当中 err = ch.Publish( \"\", // exchange \"work\", // 队列名 false, // mandatory false, // immediate amqp.Publishing &#123; ContentType: \"text/plain\", //[]byte化body Body: []byte(body), &#125;)&#125; 这样就完成了上传消息到work队列当中。 接收 Consume接收，顾名思义，就是接收到指定队列中的信息，信息存在队列当中，总要被拿出来用吧，放那里又不能下崽儿，所以，拿出来感觉用了才是最重要的。这块的基础代码如下 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( \"log\" \"github.com/streadway/amqp\")func main() &#123; //初始化一个Rabbimtq连接，后跟ip，user，password conn, err := amqp.Dial(\"amqp://guest:guest@localhost:5672/\") if err != nil &#123; return &#125; defer conn.Close() //创建一个channel的套接字连接 ch, _ := conn.Channel() msgs, err := ch.Consume( \"work\" // 队列名 \"\", // consumer true, // auto-ack false, // exclusive false, // no-local false, // 不等待 nil, // args ) //定义一个forever，让他驻留在后台，等待消息，来了就消费 forever := make(chan bool) //执行一个go func 完成任务消费 go func() &#123; for d := range msgs &#123; //打印body log.Printf(\"message %s\", d.Body) &#125; &#125;() &lt;-forever&#125; 生产者／消费者模型上面简单说了一下rabbimq的发送和接收，这下咱们就要实现一个生产者消费者模型了，这个模型的主要逻辑，就是生产者发送任务到指定的队列，有一个，或者多个消费者，会在此留守，一有任务，就争抢并且消费。 生产者逻辑其实生产者逻辑和上面的发送逻辑差不多，这里给出写法。 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( \"log\" \"github.com/streadway/amqp\")func main() &#123; //初始化一个Rabbimtq连接，后跟ip，user，password conn, err := amqp.Dial(\"amqp://guest:guest@localhost:5672/\") if err != nil &#123; return &#125; defer conn.Close() //创建一个channel的套接字连接 ch, _ := conn.Channel() //创建一个指定的队列 q, _ := ch.QueueDeclare( \"work\", // 队列名 false, // durable false, // 不使用删除？ false, // exclusive false, // 不必等待 nil, // arguments ) //定义上传的消息 body := \"work message\" //调用Publish上传消息1到指定的work队列当中 err = ch.Publish( \"\", // exchange \"work\", // 队列名 false, // mandatory false, // immediate amqp.Publishing &#123; ContentType: \"text/plain\", //[]byte化body Body: []byte(body), &#125;)&#125; 消费者逻辑消费者逻辑这边，主要是加了一个qos控制和手动ack，代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mainimport ( \"log\" \"github.com/streadway/amqp\")func main() &#123; //初始化一个Rabbimtq连接，后跟ip，user，password conn, err := amqp.Dial(\"amqp://guest:guest@localhost:5672/\") if err != nil &#123; return &#125; defer conn.Close() //创建一个channel的套接字连接 ch, _ := conn.Channel() //创建一个qos控制 err = ch.Qos( 3, // 同时最大消费数量（意思就是最多能消费几个任务） 0, // prefetch size false, // 全局设定？ ) if err != nil &#123; return err &#125; msgs, err := ch.Consume( \"work\" // 队列名 \"\", // consumer true, // auto-ack false, // exclusive false, // no-local false, // 不等待 nil, // args ) //定义一个forever，让他驻留在后台，等待消息，来了就消费 forever := make(chan bool) //执行一个go func 完成任务消费 go func() &#123; for d := range msgs &#123; //打印body log.Printf(\"message %s\", string(d.Body)) //手动ack，不管是否发送完毕。 d.Ack(false) &#125; &#125;() &lt;-forever&#125; Golang封装Rabbitmq的基础接口Rabbitmq会用了吧，上面那个估计比较简单，但是估摸着你们还想要别的功能，好，那我就惯大家一次，干了兄弟们，奥莉给！ 初始化Rabbitmq连接为了避免每次重复调用Rabbitmq连接，我这里提供一个简单写法。 1234567891011121314151617181920212223242526package mainimport (\"context\"\"fmt\"\"github.com/streadway/amqp\")//Rabbitmq 初始化rabbitmq连接type Rabbitmq struct &#123; conn *amqp.Connection err error&#125;func New(ip string) (*Rabbitmq, error) &#123; amqps := fmt.Sprintf(\"amqp://guest:guest@%s:5672/\", ip) conn, err := amqp.Dial(amqps) if err != nil &#123; return nil, err &#125; rabbitmq := &amp;Rabbitmq&#123; conn: conn, &#125; return rabbitmq, nil&#125; 创建一个Queue队列12345678910111213141516171819func (rabbitmq *Rabbitmq) CreateQueue(id string) error &#123; ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return err &#125; _, err = ch.QueueDeclare( id, // name true, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) if err != nil &#123; return err &#125; return nil&#125; 上传消息到指定的queue中123456789101112131415161718192021func (rabbitmq *Rabbitmq) PublishQueue(id string, body string) error &#123; ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return err &#125; err = ch.Publish( \"\", // exchange id, // routing key false, // mandatory false, amqp.Publishing&#123; DeliveryMode: amqp.Persistent, ContentType: \"text/plain\", Body: []byte(body), &#125;) if err != nil &#123; return err &#125; return nil&#125; 从队列中取出消息并且消费123456789101112131415161718192021func (rabbitmq *Rabbitmq) PublishQueue(id string, body string) error &#123; ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return err &#125; err = ch.Publish( \"\", // exchange id, // routing key false, // mandatory false, amqp.Publishing&#123; DeliveryMode: amqp.Persistent, ContentType: \"text/plain\", Body: []byte(body), &#125;) if err != nil &#123; return err &#125; return nil&#125; 统计队列中预备消费的数据12345678910111213func (rabbitmq *Rabbitmq) GetReadyCount(id string) (int, error) &#123; count := 0 ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return count, err &#125; state, err := ch.QueueInspect(id) if err != nil &#123; return count, err &#125; return state.Messages, nil &#125; 统计消费者／正在消费的数据12345678910111213func (rabbitmq *Rabbitmq) GetConsumCount(id string) (int, error) &#123; count := 0 ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return count, err &#125; state, err := ch.QueueInspect(id) if err != nil &#123; return count, err &#125; return state.Consumers, nil&#125; 清理队列123456789101112func (rabbitmq *Rabbitmq) ClearQueue(id string) (string, error) &#123; ch, err := rabbitmq.conn.Channel() defer ch.Close() if err != nil &#123; return \"\", err &#125; _, err = ch.QueuePurge(id, false) if err != nil &#123; return \"\", err &#125; return \"Clear queue success\", nil&#125; 总结简单讲了一下Rabbimtq是啥，怎么用，我是怎么用的。完整代码请访问我的Github： https://github.com/Alexanderklau/Go_poject/blob/master/Go-Rabbitmq/rabbitmq.go如果有不懂的欢迎留言！如果能帮大家的我一定会帮！也希望你们指出我的错误！一起进步！","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang 完成一个 Crontab定时器（2）","slug":"Golang-完成一个-Crontab定时器（2）","date":"2020-03-23T01:14:40.000Z","updated":"2020-03-23T01:15:19.158Z","comments":true,"path":"2020/03/23/Golang-完成一个-Crontab定时器（2）/","link":"","permalink":"https://yemilice.com/2020/03/23/Golang-%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA-Crontab%E5%AE%9A%E6%97%B6%E5%99%A8%EF%BC%882%EF%BC%89/","excerpt":"","text":"前言上篇文章，大概讲了一下robfig/cron 包的使用，怎么开始一个定时任务，那个东西比较简单，也就是调用函数而已，人家都给你把包都封装好了。鉴于上一章我没提到cron相关，这一章专门我写个cron相关，讲讲怎么cron语法，然后再实现一个自动生成cron语句的逻辑。 需求分析 cron的基础科普 根据时间自动生成可用的cron语句 Cron表达式的基础Go的Cron和linux的Cron的区别就是，linux只到分钟，但是Go的Cron可以通过我上一节描述的代码设置精确到秒。所以一般的Cron表达式就是 1* * * * * * command 可以看出来，这是一个时间集合，但是其中每个 * 代表什么含义呢？下面给出Golang的cron设置表 字段 需要的值 字符表示 秒 0-59 * / , - 分 0-59 * / , - 时 0-23 * / , - 日 1-31 * / , - 月 1-12 * / , - 星期 0-6 * / , - 下面举几个cron的具体例子每秒执行一次任务 1* * * * * * Command 每分钟执行一次任务 1* *／1 * * * * Command 每天12点执行一次任务 1* 0 12 * * * Command 每个月1号12点执行一次任务 1* 0 12 1 * * Command 2月14号12点执行一次任务（执行一次） 10 0 12 14 2 * Command 每周二12点执行一次任务 1* 0 12 * * 1 Command Golang 实现一个Cron表达式自动生成器Cron这个东西，其实没那么难，但是你每次让我们徒手撸，还是会有点烦，特别是现在网上基本没有在线自动生成Cron语法的网站了，所以我们还是站撸一个Cron自动生成器，首先咱们要明确一个重要东西,任务可能是循环的，也可能是只执行一次的，看到了么，这下我们就要针对不同的任务类型，输出不同的任务表达式。 规定输入的时间格式首先输入的时间有多种多样，我们没办法控制输入的时间表达，所以我在这里先行规定，我的代码也是按照这个规定来的，前提在此。 循环执行的任务对于循环执行的任务，可能有每月，每周，每日，每时等等，所以我在这里举例 每月3号12点执行 1m,03,12:00 每周三的12点执行 1w,3,12:00 每天的12点执行 1d,12:00 观察一下，聪明的你应该知道我要做什么，拿每天循环执行来举例 12345timelists := strings.Split(times, \",\")hours := strings.Split(timelists[1], \":\")[0]minutes := strings.Split(timelists[1], \":\")[1]crontab := fmt.Sprintf(\"* %s %s * * *\", minutes, hours)fmt.Println(crontab) 结合其他部分 1234567891011121314151617181920212223timelists := strings.Split(times, \",\")// 在这里判断类型，天，月，周if timelists[0] == \"d\" &#123; hours := strings.Split(timelists[1], \":\")[0] minutes := strings.Split(timelists[1], \":\")[1] crontab := fmt.Sprintf(\"* %s %s * * *\", minutes, hours) return crontab&#125; else if timelists[0] == \"w\" &#123; days := strings.Split(timelists[1], \",\")[0] hours := strings.Split(strings.Split(timelists[2], \",\")[0], \":\")[0] minutes := strings.Split(strings.Split(timelists[2], \",\")[0], \":\")[1] crontab := fmt.Sprintf(\"* %s %s * * %s\", minutes, hours, days) return crontab&#125; else if timelists[0] == \"m\" &#123; days := strings.Split(timelists[1], \",\")[0] hours := strings.Split(strings.Split(timelists[2], \",\")[0], \":\")[0] minutes := strings.Split(strings.Split(timelists[2], \",\")[0], \":\")[1] crontab := fmt.Sprintf(\"* %s %s %s * *\", minutes, hours, days) return crontab&#125; else &#123; crontab := \"* * * * * *\" return crontab&#125; 执行一发看看,生成个每月的cron表达式 1* 0 12 03 * * Command 诶，怎么多了个03。。。看起来咱们需要格式化一下，把它转换一下成可用的。 1234567891011121314func FkZero(times string) (fmttime string) &#123; // 如果第一个值不为0，直接认为是正常的 if string(times[0]) != \"0\" &#123; return times // 判断00的情况 &#125; else if strings.Split(times, \"0\")[1] == \"\" &#123; fkzero := \"0\" return fkzero // 清理03，为 3 &#125; else &#123; fkzero := strings.Split(times, \"0\")[1] return fkzero &#125;&#125; 执行一次的任务执行一次的任务表达方式 2020-03-20 12:00处理代码如下 12345678timelists := strings.Split(times, \" \")[0]month := strings.Split(timelists, \"-\")[1]day := strings.Split(timelists, \"-\")[2]timework := strings.Split(times, \" \")[1]hours := strings.Split(timework, \":\")[0]minutes := strings.Split(timework, \":\")[1]crontab := fmt.Sprintf(\"0 %s %s %s %s *\", minutes, hours, day, month)return crontab 总结其实这个代码主要就是一个strings的split切分，但是涉及到了crontab语言的输出，其实没那么难，也就是麻烦，我把它传到github上了，有需要可以自己get下来。https://github.com/Alexanderklau/Go_poject/tree/master/Go-Script/crontab","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang 完成一个 Crontab定时器（1）","slug":"Golang-完成一个-Crontab定时器（1）","date":"2020-03-23T01:12:36.000Z","updated":"2020-03-23T01:13:34.795Z","comments":true,"path":"2020/03/23/Golang-完成一个-Crontab定时器（1）/","link":"","permalink":"https://yemilice.com/2020/03/23/Golang-%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA-Crontab%E5%AE%9A%E6%97%B6%E5%99%A8%EF%BC%881%EF%BC%89/","excerpt":"","text":"前言Linux的Crontab定时器似乎已经足够强大，但是我认为还是没有办法满足我们所有的需求，例如定时器某一瞬间需要动态添加／删除任务的功能，例如定时器只能在指定的节点上启动（主节点），其他节点不需要定时服务，这种情况Linux自带的Crontab就不能够满足我们的需求了，所以这次要徒手定义一个Crontab定时器，作为自己的备用。 需求分析看我博客的基本也都知道，做任何事，都要进行一个需求分析。既然是一个定时器，那么应该支持的功能如下 定时启动任务（废话） 支持基础的Crontab语法 支持将时间转换为Crontab语法 支持Crontab语法校验 记录日志的功能 支持crontab任务到秒级 综合上面的需求，不难看出，其实最重要的功能就是实现一个定时启动任务的玩意儿，在Go开发当中，也就是在某个时间点执行某个Go函数。说的简单点，就是，定时跑Go函数。也就是定时go func而已。 robfig/cron 包的使用安装robfig/cron包我这里默认你们都有go 基础,先安装个包，这玩意儿是驱动Golang 驱动 Crontab的重要框架。 1go get github.com/robfig/cron robfig/cron的使用举例启动一个定时任务其实很简单 123job := cron.New()job.AddFunc(\"* * * * *\", func() &#123;fmt.Println(\"Start job....\")&#125;)job.Start() 一个定时任务就这样被写好了，其实仔细琢磨一下，添加任务的方式就是 123456// 新任务job := cron.New()///任务添加job.AddFunc(\"Cronta 语句\", func() &#123;执行函数（）&#125;)//任务开始job.Start() 这样就完成了一个定时任务的添加 支持crontab任务到秒级估计你们看到这里一脸懵逼，为啥你个单独到秒级的也要整出来呢？其实我也表示，我也不想啊！奈何一点，robfig/cron这玩意有个很奇葩的一点，它只支持的分钟级别的任务，不支持到秒级别！！！！，它只支持的分钟级别的任务，不支持到秒级别！！！！，它只支持的分钟级别的任务，不支持到秒级别！！！！，重要的事儿说三遍！这里需要你自己定义秒级别的任务，在翻了一下它的test源码之后，拉到最底下，看到这么一行代码。 1234// newWithSeconds returns a Cron with the seconds field enabled.func newWithSeconds() *Cron &#123; return New(WithParser(secondParser), WithChain())&#125; 这行代码啥意思呢，意思就是启用返回seconds字段的任务，说白了就是，你要加这个，才能开启秒级别的任务网上好多博客写的，都是你抄我，我抄你，抄来抄去，没一个代码能用的，大家如果发现抄网上那帮人的代码，跑不起来，那绝对就是这个原因！人家只支持到分钟，网上给出的例子全都是秒级别的，并且没打开秒级别任务定义，还能跑起来？我都怀疑你们怎么写的代码。定义秒级别任务代码这段代码主要的意思就是，开放到秒级别的任务支持，看到了second么，这段代码在源码包的test下有，你们可以自己去看看。 1234567891011121314151617181920func newWithSeconds() *cron.Cron &#123; secondParser := cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.DowOptional | cron.Descriptor) return cron.New(cron.WithParser(secondParser), cron.WithChain())&#125;func main() &#123; // 使用秒级别任务 job := newWithSeconds() // 任务定义,3s输出一个j1 start job,不停循环 job.AddFunc(\"0/3 * * * * ? \", func() &#123; fmt.Println(\"j1 start job....\", time.Now().Format(\"2020-03-20 15:04:05\")) &#125;) // 任务定义，每分钟的第三秒执行任务 job.AddFunc(\"3 * * * * ? \", func() &#123; fmt.Println(\"j2 start job....\", time.Now().Format(\"2020-03-20 15:04:05\")) &#125;) //开始任务 job.Start() select &#123;&#125;&#125; 输出的结果 1234j1 start job.... 2020-03-21 17:09:36j1 start job.... 2020-03-21 17:09:39j1 start job.... 2020-03-21 17:09:42j1 start job.... 2020-03-21 17:09:45 总结初步完成了crontab的基础功能，这篇文章默认，你们都比较懂crontab语法和go开发了，如果不懂就请期待下一篇，实现crontab语法自动生成和自动加载任务吧。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang利用context实现一个任务并发框架","slug":"Golang利用context实现一个任务并发框架","date":"2020-03-20T07:57:48.000Z","updated":"2020-05-19T08:28:05.669Z","comments":true,"path":"2020/03/20/Golang利用context实现一个任务并发框架/","link":"","permalink":"https://yemilice.com/2020/03/20/Golang%E5%88%A9%E7%94%A8context%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%8F%91%E6%A1%86%E6%9E%B6/","excerpt":"","text":"Golang利用context实现一个任务并发框架消失这么久的原因疫情太严重，哥们本来打算在新疆滑雪+吃烤肉度过一个美好的假期，结果没成想给困那里了，这不就尴尬了么，这不，博客没更新，现在我又回来了，哈哈哈哈！ 我要实现个什么玩意儿有一个需求，简单的说就是我要写一个任务管理框架，主要功能有任务开启，任务关闭，任务监控等等。说的抽象点，就是我要用Golang写一个任务管理的功能，任务很可能有多个，并且我想停任务就停任务，想开始任务我就开始任务！我不要你觉得，我要我觉得！ 为什么我要用Golang众所周知，golang这东西，有个黑科技，叫goroutine，这东西很牛逼，牛逼在哪儿呢？简单的说，快，小，短！协程切换快，占用资源少，并且，异步的，可以开多个，直接一个go 关键字就给人家打开了，多棒！ 实际需求分析结合我们上面的任务需求，实现一个任务开始，任务停止的逻辑，这就说明，任务肯定不只有一个，并且任务都在后台，我们该怎么去监控，或者去管理这个go任务，或者go函数呢，golang提供了很多解决办法，例如WaitGroup，context等方法。思考一下，我们的这个需求，任务都是跑在后台的异步并发逻辑，这就说明不只一个任务会被启动和停止，这样对我们的任务管理是一个很大的挑战，因为任务都是在后台隐秘执行的，如果是一般逻辑，我们要停止任务，首先要找到任务的pid，然后kill任务进程，这是一个完整的结束任务的流程。回到我们这个需求，基本的流程就是： 发送一个任务请求(开启任务/停止任务) -&gt; 接收到任务请求 -&gt; 执行任务请求 思考一下，如果，我们开启任务之后，任务进入后台，那么，我们在停止任务的时候，怎么保证，能够找到这个任务，精准的打击（停止）它呢？看了题目你应该就知道了，用context就好了。下面我就来介绍一下它吧。 主角context介绍网上很多博客介绍它，我粗粗看了一眼，非常抽象，很多人再一描述，就更麻烦更抽象了。我这里不说的太麻烦，简单描述一下，这个东西context，是干嘛呢，你们理解一下株连，连坐这两个词汇，这个东西相当于就是锁链，铁锁连舟，不进则退，说明白点，就是一个串连上下文的类似信号传递的玩意儿。每个调用链上的函数都要以它作为函数进行传递，举个例子,”株九族”这个词，是因为一个人犯罪，结果家人都因为他被砍了头，这个犯罪的人，就是父context，其他因为他被杀的人，就是子context，还可能有孙context，他被砍头了，其他人也得跟着一起死，用代码表示一波 12345678910111213141516171819202122232425262728// 这是儿子函数func gen(ctx context.Context) &lt;-chan int &#123; dst := make(chan int) n := 1 go func() &#123; for &#123; select &#123; // 接收到爹挂了的消息 case &lt;-ctx.Done(): fmt.Println(\"儿子被砍头了。\") // 退出任务 return case dst &lt;- n: n++ time.Sleep(time.Second * 1) &#125; &#125; &#125;() return dst&#125;// 这是爹函数fun test() &#123; ctx, cancel := context.WithCancel(context.Background()) // 让我造个儿子，给我儿子传个ctx intChan := gen(ctx) // 我被干了，cancel是结束 defer Cancel()&#125; 这下说的明白了么？其实context还有很多别的，例如timeout之类的，但是那个是我后面准备写的，这一节就不写这些了。 实现我们的需求我们的武器context已经准备好了，大概的使用逻辑我们也明白了，现在你们可以看到，我们只要拿到主函数（爹函数）的ctx和cancel，我们就可以控制子函数（儿子函数）的死活，我们在开发当中，任务的状态是不断在变化的，一个爹对应一个儿子，但是可能有多个任务，多个任务我们该怎么管理它？在一般的开发任务中，我们习惯将任务记录到数据库当中，然后在开发当中不停的遍历数据库，去判断任务的状态到底是开启还是停止，这里我们要考虑到，频繁遍历数据库，会不会带来大量的访问堆积？还是否有别的解决办法？ 我的解决方案这次开发中，我选择定义一个全局变量的主map，并且定义一个任务的struct类型，代码如下 123456789// 全局mapvar jobmap = make(map[string]interface&#123;&#125;)//Job 任务type Jobs struct &#123; ID string Status int Ctx context.Context Cancel context.CancelFunc&#125; 然后我选择在任务开始的时候（创造儿子的时候），将信息填充，修改test代码如下 123456789101112131415161718192021func test() &#123; var jobs Jobs ctx, cancel := context.WithCancel(context.Background()) // 造一个儿子 intChan := gen(ctx) // 任务开始了 fmt.Println(\"start job\") // 重要的东西传进去 jobs.Status = 1 jobs.Cancel = cancel jobs.Ctx = ctx // 定义一个任务id，这个可以用uuid，或者随便整个别的 jobs.ID = \"sdads\" m1[\"sdads\"] = jobs // 阻塞任务，假装任务执行很久 for n := range intChan &#123; fmt.Println(n) if n == 1000 &#123; break &#125; &#125; 再然后，我选择写一个停止函数（砍头函数） 123456789func stopGetmi(id string) &#123; //把任务停掉 fmt.Println(\"stop jobs\") jobss := m1[id] //interface 转 struct op, ok := jobss.(Jobs) // 调用砍头函数cancel defer op.Cancel()&#125; 进行测试 12345func main() &#123; go test() go stopGetmi(\"sdads\") time.Sleep(time.Second * 200)&#125; 发现任务执行结果这样你就完成了干掉老爹，也干掉儿子的素质操作。 总结主要是context的基础和说明，其实context我还是推荐大家去看看原版，我这里写的太过于简单，不过这篇博客，也是我记录一下自己开发中遇到的难题，当时看网上没有类似的说明，于是写了这篇博客，希望大家多多包涵。祝大家都能躲过瘟疫，我们终究会在春花花开的地方相见。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"ETCD分布式锁实现选主机制(Golang)","slug":"ETCD分布式锁实现选主机制-Golang","date":"2019-12-13T07:41:04.000Z","updated":"2019-12-13T07:44:02.059Z","comments":true,"path":"2019/12/13/ETCD分布式锁实现选主机制-Golang/","link":"","permalink":"https://yemilice.com/2019/12/13/ETCD%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E9%80%89%E4%B8%BB%E6%9C%BA%E5%88%B6-Golang/","excerpt":"","text":"ETCD分布式锁实现选主机制(Golang)为什么要写这篇文章做架构的时候，涉及到系统的一个功能，有一个服务必须在指定的节点执行，并且需要有个节点来做任务分发，想了半天，那就搞个主节点做这事呗，所以就有了这篇文章的诞生，我把踩的坑和收获记录下来，方便未来查看和各位兄弟们参考。 选主机制是什么举个例子，分布式系统内，好几台机器，总得分个三六九等，发号施令的时候总得有个带头大哥站出来，告诉其他小弟我们今天要干嘛干嘛之类的，这个大哥就是master节点，master节点一般都是做信息处理分发，或者重要服务运行之类的。所以，选主机制就是，选一个master出来，这个master可用，并且可以顺利发消息给其他小弟，其他小弟也认为你是master，就可以了。 ETCD的分布式锁是什么首先认为一点，它是唯一的，全局的，一个key值为什么一定要强调这个唯一和全局呢，因为分布式锁就是指定只能让一个客户端访问这个key值，其他的没法访问，这样才能保证它的唯一性。再一个，认为分布式锁是一个限时的，会过期的的key值你创建了一个key，要保证访问它的客户端时刻online，类似一个“心跳”的机制，如果持有锁的客户端崩溃了，那么key值在过期后会被删除，其他的客户端也可以继续抢key，继续接力，实现高可用。 选主机制怎么设计其实主要的逻辑前面都说清楚了，我在这里叙述下我该怎么做。我们假设有三个节点，node1,node2,node3 三个节点都去创建一个全局的唯一key /dev/lock 谁先创建成功谁就是master主节点 其他节点持续待命继续获取，主节点继续续租key值（key值会过期） 持有key的节点down机，key值过期被删，其他节点创key成功，继续接力。ETCD分布式锁简单实现看一下ETCD的golang代码，还是给出了如何去实现一个分布式锁，这个比较简单，我先写一个简单的Demo说下几个接口的功能 创建锁123456789kv = clientv3.NewKV(client)txn = kv.Txn(context.TODO())txn.If(clientv3.Compare(clientv3.CreateRevision(\"/dev/lock\"),\"=\",0)).Then(clientv3.OpPut(\"/dev/lock\",\"占用\",clientv3.WithLease(leaseId))).Else(clientv3.OpGet(\"/dev/lock\"))txnResponse,err = txn.Commit()if err !=nil&#123; fmt.Println(err) return &#125; 判断是否抢到锁12345if txnResponse.Succeeded &#123; fmt.Println(\"抢到锁了\") &#125; else &#123; fmt.Println(\"没抢到锁\",txnResponse.Responses[0].GetResponseRange().Kvs[0].Value) &#125; 续租逻辑1234567891011for &#123; select &#123; case leaseKeepAliveResponse = &lt;-leaseKeepAliveChan: if leaseKeepAliveResponse != nil&#123; fmt.Println(\"续租成功,leaseID :\",leaseKeepAliveResponse.ID) &#125;else &#123; fmt.Println(\"续租失败\") &#125; &#125; time.Sleep(time.Second*1) &#125; 我的实现逻辑首先我的逻辑就是，大家一起抢，谁抢到谁就一直续，要是不续了就另外的老哥上，能者居之嘛！我上一下我的实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116package mainimport ( \"fmt\" \"context\" \"time\" //\"reflect\" \"go.etcd.io/etcd/clientv3\")var ( lease clientv3.Lease ctx context.Context cancelFunc context.CancelFunc leaseId clientv3.LeaseID leaseGrantResponse *clientv3.LeaseGrantResponse leaseKeepAliveChan &lt;-chan *clientv3.LeaseKeepAliveResponse leaseKeepAliveResponse *clientv3.LeaseKeepAliveResponse txn clientv3.Txn txnResponse *clientv3.TxnResponse kv clientv3.KV)type ETCD struct &#123; client *clientv3.Client cfg clientv3.Config err error&#125;// 创建ETCD连接服务func New(endpoints ...string) (*ETCD, error) &#123; cfg := clientv3.Config&#123; Endpoints: endpoints, DialTimeout: time.Second * 5, &#125; client, err := clientv3.New(cfg) if err != nil &#123; fmt.Println(\"连接ETCD失败\") return nil, err &#125; etcd := &amp;ETCD&#123; cfg: cfg, client: client, &#125; fmt.Println(\"连接ETCD成功\") return etcd, nil&#125;// 抢锁逻辑func (etcd *ETCD) Newleases_lock(ip string) (error) &#123; lease := clientv3.NewLease(etcd.client) leaseGrantResponse, err := lease.Grant(context.TODO(), 5) if err != nil &#123; fmt.Println(err) return err &#125; leaseId := leaseGrantResponse.ID ctx, cancelFunc := context.WithCancel(context.TODO()) defer cancelFunc() defer lease.Revoke(context.TODO(), leaseId) leaseKeepAliveChan, err := lease.KeepAlive(ctx, leaseId) if err != nil &#123; fmt.Println(err) return err &#125; // 初始化锁 kv := clientv3.NewKV(etcd.client) txn := kv.Txn(context.TODO()) txn.If(clientv3.Compare(clientv3.CreateRevision(\"/dev/lock\"), \"=\", 0)).Then( clientv3.OpPut(\"/dev/lock\", ip, clientv3.WithLease(leaseId))).Else( clientv3.OpGet(\"/dev/lock\")) txnResponse, err := txn.Commit() if err != nil &#123; fmt.Println(err) return err &#125; // 判断是否抢锁成功 if txnResponse.Succeeded &#123; fmt.Println(\"抢到锁了\") fmt.Println(\"选定主节点\", ip) // 续租节点 for &#123; select &#123; case leaseKeepAliveResponse = &lt;-leaseKeepAliveChan: if leaseKeepAliveResponse != nil &#123; fmt.Println(\"续租成功,leaseID :\", leaseKeepAliveResponse.ID) &#125; else &#123; fmt.Println(\"续租失败\") &#125; &#125; &#125; &#125; else &#123; // 继续回头去抢，不停请求 fmt.Println(\"没抢到锁\", txnResponse.Responses[0].GetResponseRange().Kvs[0].Value) fmt.Println(\"继续抢\") time.Sleep(time.Second * 1) &#125; return nil&#125;func main()&#123; // 连接ETCD etcd, err := New(\"xxxxxxxx:2379\") if err != nil &#123; fmt.Println(err) &#125; // 设定无限循环 for &#123; etcd.Newleases_lock(\"node1\") &#125;&#125; 总结相关代码写入到github当中，其中的地址是https://github.com/Alexanderklau/Go_poject/tree/master/Go-Etcd/lock_work实现这个功能废了不少功夫，好久没写go了，自己太菜了，如果有老哥发现问题请联系我，好改正。","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"算法笔记-入门-数据结构篇","slug":"算法笔记-入门-数据结构篇","date":"2019-11-13T03:26:18.000Z","updated":"2020-05-19T07:42:52.708Z","comments":true,"path":"2019/11/13/算法笔记-入门-数据结构篇/","link":"","permalink":"https://yemilice.com/2019/11/13/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AF%87/","excerpt":"","text":"算法笔记-入门-数据结构篇从大学毕业之后就没研究过算法，都快忘光了，现在开个新坑，从头学起算法，哈哈，希望自己能够坚持住，不过我一定可以坚持住的，我就像易筋洗髓一样，将自己全身打断，重塑自己的一切，回归初心，以一个听者的名义对待一切，因为我做的都是我自己喜欢的事儿。 基本的数据结构类型什么是数据结构说白了很简单，数据存计算机里，总得有个存放规律，不能乱来，就像你查字典，你可以一页一页翻着找字儿，你也可以直接按拼音跳转找字儿，这就是查字典的数据结构，数据结构就是决定数据顺序和位置的关系。 数据结构的子类-链表链表，这玩意儿理解起来会抽象一些，大学课本上表示它的数据是一个线性排列的，要我说不用这么麻烦，链表其实就是一列火车，举例来说，现在有4节车厢，你必须通过一节车厢才能到下一节去，也就是说，车厢（链表）都有一个指示牌（指针），你必须一个个往下，到达下面的车厢（指向下一个地址）。链表这玩意儿吧，慢，查东西你得一个个往下，添加，删除数据都先要改变指针。查一个东西，拿大O表示法，它的复杂度是O(n)，算是相当慢的一种算法了。 数据结构的子类-数组数组，也是线性排列的数据结构，还记得链表么，链表是靠指针指向，告诉你我下一个老哥是谁，但是数组不一样，它是靠一个叫数组下标的东西来告诉你，我是第几个，在Python中，这玩意儿被运用在列表里，就像a = [1,2,3,4,5] 这种形式，a[0] = 1, a[1] = 2…….其实你想查一个数组里面的东西，一般都是随机访问，可以直接去访问数组下标，这东西就相当于你吃饭取的号儿，到你了，人家就喊“XXX号用餐了！”数组下标就这个功能。数组里面，你要添加或者删除一个元素，那可有点麻烦，你要先在数组尾部，加一个多的存储空间，总不可能让新元素没地方去吧，然后你要让旧元素给新朋友腾个位置，然后把旧朋友往后面赶，然后新朋友才能顺利插队。。。 数据结构的子类-栈栈这位老哥会理解麻烦一点，这么想，你随时随地都能吃到最新鲜的水果，每天都有新的水果，你总能拿到新水果，旧水果就只有在下面，所以你如果想吃旧水果，你就要把水果箱子一点点拿出来，被称为出栈，把水果放回去，叫进栈，这个东西就是你只能拿最新的，后进先出，LIFO结构，这玩意儿还是挺不方便的。不过在业务中，如果你需要时刻保持最新数据在前面，例如，时事热点，附近的人等等，拿这玩意儿就好用多了。 数据结构的子类-队列和上面的栈老哥相反，队列的意思就是，你先进来的啊，你边儿待着，该需要的时候要我旧数据先上，拿数据从最老的数据拿，新数据一边玩去。想拿新数据？不好意思，一个个出来吧你，直到该你出去为止。 数据结构的子类-哈希表这个在这说有点那啥，但是啊，你们写过Python的应该知到，Python里面有个dict（字典），字典这玩意儿就是一个key 对应 一个value，例如 1234&#123; &quot;蔡徐坤&quot; ：&quot;篮球&quot;， &quot;吴亦凡&quot; ：&quot;说唱&quot;&#125; 这就实现了一个字典，你会问，这和TM哈希表有啥关系，哈希表这玩意儿，是存dict的东西，它是个类数组的东西，但是存的东西很变态，一般我们会用hash函数，计算”蔡徐坤”的键,也就是”蔡徐坤”的哈希值，然后我们将得到的哈希值除以数组长度（哈希表长）取余数，计算出”蔡徐坤”在数组中的位置，然后把它放进去。这里比较抽象，后面会专门讲一下哈希这个算法。那么怎么查呢，首先我们拿到”蔡徐坤”的哈希值，然后通过刚才的计算就能找到”蔡徐坤”在哈希表中的位置了。 数据结构的子类-堆重头戏来了，这玩意儿是一个树形结构，树形，顾名思义，是分叉的，想象一下，一棵树的样子，这玩意儿是拿来搞优先队列用的，堆里面有个老大，叫结点，所有的数据都是结点的小弟，受他罩着，其中他有多个手下，叫子结点，子结点一般比父结点大，最小的值一般都在堆的顶点。堆中最顶端的数据始终最小，所以无论数据量有多少，取出最小值的时间复杂度都 为 O(1)。另外，因为取出数据后需要将最后的数据移到最顶端，然后一边比较它与子结点数据 的大小，一边往下移动，所以取出数据需要的运行时间和树的高度成正比。假设数据量为 n，根据堆的形状特点可知树的高度为 log2n ，那么重构树的时间复杂度便为 O(logn)。添加数据也一样。在堆的最后添加数据后，数据会一边比较它与父结点数据的大 小，一边往上移动，直到满足堆的条件为止，所以添加数据需要的运行时间与树的高度 成正比，也是 O(logn） 数据结构的子类-二叉查找树和楼上那位一样，也是个树形结构，不一样的是，二叉树每个结点的值大于左子树上任意一个结点的值，比较抽象对吧，来看个图意思就是，无论怎么样，左边老哥总会比我小。第二个分歧就是，右边老哥总比我大，继续看图这就是一部分基础，做了粗略写作，写得不好，见谅，over！","categories":[],"tags":[{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"},{"name":"算法","slug":"算法","permalink":"https://yemilice.com/tags/%E7%AE%97%E6%B3%95/"}],"keywords":[]},{"title":"Python高效率遍历文件夹寻找重复文件","slug":"Python高效率遍历文件夹寻找重复文件","date":"2019-10-28T04:31:15.000Z","updated":"2019-10-28T04:59:05.962Z","comments":true,"path":"2019/10/28/Python高效率遍历文件夹寻找重复文件/","link":"","permalink":"https://yemilice.com/2019/10/28/Python%E9%AB%98%E6%95%88%E7%8E%87%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AF%BB%E6%89%BE%E9%87%8D%E5%A4%8D%E6%96%87%E4%BB%B6/","excerpt":"","text":"前言为什么要写这篇文章呢。。。主要还是业务中有个需求，遍历一个将近200w数据的文件夹，大部分还都是视频文件那种，但是这玩意用的次数还不多，做文件夹index也不是很ok，所以写了一个脚本来处理这个问题，从而发现了自己的一些薄弱点，将其记录下来，方便自己，也方便未来其他的兄弟使用 基本需求 把文件夹中的重复文件找出来 找出来之后用csv输出，左边是源文件，右边是重复文件 效率不能差，不能直接撑爆内存，不能占用过多资源 检测的文件夹和存放csv的地方可以自己定义，加上终端交互 重复文件筛选支持md5，大小等方式需求分析首先要分析一点，就是我们该如何去做重复文件的对比，并且效率还要高，首先网上过多的递归，os.walk的方法不可用，因为他们都会把遍历到的内容直接做成一个大列表，塞到内存里面，数据量大很容易爆掉，并且还要进行MD5，或者是大小比对，这个就非常难缠了。基础想法其实说白了，拿到所有文件列表file_list，把文件依次对比，这里我们可以用dict，分两种情况 按照文件名和大小设定两个dict，例如record和dup，遍历file_list,生成一个数组，比对其中的文件名和大小按照大小和MD5值设定两个dict，例如record和dup，遍历file_list,生成一个数组，比对其中的md5值和大小具体代码闲话休提，我们开始写代码吧定义遍历函数代码首先定义遍历文件夹的部分diskwalk.py 12345678910111213141516# coding: utf-8__author__ = \"lau.wenbo\"import os,sysclass diskwalk(object): def __init__(self, path): self.path = path def paths(self): path = self.path # 这里用了一个迭代器逻辑，防止所有数据塞内存爆掉 path_collection = (os.path.join(root,fn) for root,dirs,files in os.walk(path) for fn in files) return path_collection 定义检查md5值代码接着我们定义检查md5值的一个逻辑checksum.py 12345678910111213141516171819# coding: utf-8__author__ = \"lau.wenbo\"import hashlib,sys# 分块读MD，速度快def create_checksum(path): fp = open(path) checksum = hashlib.md5() while True: buffer = fp.read(8192) if not buffer: break checksum.update(buffer) fp.close() checksum = checksum.digest() return checksum 定义主函数代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# coding: utf-8__author__ = \"lau.wenbo\"from checksum import create_checksumfrom diskwalk import diskwalkfrom os.path import getsizeimport csvimport osimport sysreload(sys)sys.setdefaultencoding('utf8')def findDupes(path): record = &#123;&#125; dup = &#123;&#125; d = diskwalk(path) files = d.paths() for file in files: try: # 这里使用了大小，文件名的对比方式，如果你需要MD5值的对比方式，可以打开下面的注释 #compound_key = (getsize(file),create_checksum(file)) compound_key = (getsize(file), file.split(\"/\")[-1]) if compound_key in record: dup[file] = record[compound_key] else: record[compound_key]=file except: continue return dupif __name__ == '__main__': path = sys.argv[1] csv_path = sys.argv[2] if not os.path.isdir(path) or not os.path.isdir(csv_path) or csv_path[-1] != \"/\": print u\"参数不是一个有效的文件夹！\" exit() else: path = path.decode(\"utf-8\") print u\"待检测的文件夹为&#123;path&#125;\".format(path=path) with open(u\"&#123;csv_path&#125;重复文件.csv\".format(csv_path=csv_path),\"w+\") as csvfile: # 源文件 重复文件 header = [\"Source\", \"Duplicate\"] writer = csv.DictWriter(csvfile, fieldnames=header) writer.writeheader() print u\"开始遍历文件夹，寻找重复文件，请等待.........\" print u\"开始写入CSV文件，请等待........\" for file in findDupes(path).items(): writer.writerow(&#123;\"Source\":file[1],\"Duplicate\":file[0]&#125;) 结语实现了哪些功能呢，哈哈，结尾来说一下，其实核心就是我用了一个列表生成器，加了一个迭代器，迭代器可是好东西，不会撑内存，不错了，效率也还可以，200w数据判定也就20多分钟，支持大数据量，如果有什么不懂的，可以邮件联系我或者等待我的评论系统搞完，overgithub地址在这: https://github.com/Alexanderklau/Amusing_python/tree/master/File_operation/repeat","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"},{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]},{"title":"Elasticsearch for python API模块化封装","slug":"Elasticsearch-for-python-API模块化封装","date":"2019-10-25T01:57:50.000Z","updated":"2020-04-14T12:03:27.332Z","comments":true,"path":"2019/10/25/Elasticsearch-for-python-API模块化封装/","link":"","permalink":"https://yemilice.com/2019/10/25/Elasticsearch-for-python-API%E6%A8%A1%E5%9D%97%E5%8C%96%E5%B0%81%E8%A3%85/","excerpt":"","text":"Elasticsearch for python API模块化封装模块的具体功能 检测Elasticsearch节点是否畅通 查询Elasticsearch节点健康状态 查询包含的关键字的日志（展示前10条） 查询指定的索引下的数据，并且分页 输出所有日志(输出全部) 输出去重后的日志(分页，带关键字） 删除指定索引的值 往索引中添加数据 获取指定index、type、id对应的数据 更新指定index、type、id所对应的数据 批量插入数据 使用方法一般作为独立的包进行导入，并且对其进行了大数据预览的优化和处理作为一个独立Python模块进行导入，并且调取接口使用。调用方法 12import elasticdb.es_sysdb as esesdb = es.Es() 使用举例打印出索引（表）内的所有数据：需要index名，也就是指定索引名，在这里，假设我要查所有的monlog数据，那么查询语句如: 123a = esdb.search_all(client=esdb.conn, index=monlog, type=\"doc\")for i in a: c.append(i[\"_source\"][\"message\"]) 接口详情接口参数说明 参数 必选 类型 说明 index ture str 索引名 ，可认为是数据库 type true str 索引类型，可认为是表名 keywords ture str 关键字 page ture str 页数，分页逻辑 size ture str 每页展示条数，分页逻辑使用 查询包含的关键字的日志（展示前10条）123a = esdb.search_searchdoc(index=monlog, type=\"doc\", keywords=\"cpu\")for i in a: print i[\"_source\"][\"message\"] 查询指定的索引下的数据，并且分页示例：查询index为”oplog-2018-08,oplog-2018-12”，并且每页展示（size）5条，输出第二页（page） 12for i in esdb.serch_by_index(index=\"oplog-2018-08,oplog-2018-12\", page=2, size=5)[\"hits\"][\"hits\"]: print(i[\"_source\"][\"message\"]) 输出所有日志(输出全部)12for i in esdb.search_all(client=esdb.conn, index=\"monlog-*\", type=\"doc\"): print i 输出去重后的日志(分页，带关键字）示例：关键字为空，搜索monlog的所有数据，展示第一页，并且每页展示10条 12for i in esdb.serch_es_count(keywords = \"\", index=\"monlog-*\", type=\"doc\",page=1, size=10): print i 删除指定索引的值示例：删除monlog的所有值 1esdb.delete_all_index(index=\"monlog-*\", type=\"doc\") 查询集群健康状态1esdb.check_health() 往索引中添加数据12body = &#123;\"name\": 'lucy2', 'sex': 'female', 'age': 10&#125;print esdb.insertDocument(index='demo', type='test', body=body) 获取指定index、type、id对应的数据1print esdb.getDocById(index='demo', type='test', id='6gsqT2ABSm0tVgi2UWls') 更新指定index、type、id所对应的数据12body = &#123;\"doc\": &#123;\"name\": 'jackaaa'&#125;&#125;#修改部分字段print esdb.updateDocById('demo', 'test', 'z', body) 批量插入数据12345678910_index = 'demo'_type = 'test_df'import pandas as pdframe = pd.DataFrame(&#123;'name': ['tomaaa', 'tombbb', 'tomccc'], 'sex': ['male', 'famale', 'famale'], 'age': [3, 6, 9], 'address': [u'合肥', u'芜湖', u'安徽']&#125;)print esAction.insertDataFrame(_index, _type, frame) 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344from elasticsearch import Elasticsearchfrom elasticsearch import helpersclass Es: def __init__(self): self.hosts = \"127.0.0.1\" self.conn = Elasticsearch(hosts=self.hosts, port=9200) def check(self): ''' 输出当前系统的ES信息 ''' return self.conn.info() def ping(self): return self.conn.ping() def check_health(self): ''' 检查集群的健康状态 :return: ''' status = self.conn.transport.perform_request('GET', '/_cluster/health', params=None)[\"status\"] return statuu def get_index(self): return self.conn.indices.get_alias(\"*\") def search_specify(self, index=None, type=None, keywords=None, page=None, size=None): # 查询包含的关键字的日志 query = &#123; 'query': &#123; 'match': &#123; 'message': keywords &#125; &#125;, 'from':page * size, 'size':size &#125; message = self.searchDoc(index, type, query) return message 完整的代码地址：https://github.com/Alexanderklau/elasticdb","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"Golang 调用 aws-sdk 操作 S3对象存储","slug":"Golang-调用-aws-sdk-操作-S3对象存储","date":"2019-10-25T01:55:52.000Z","updated":"2019-10-25T02:33:06.939Z","comments":true,"path":"2019/10/25/Golang-调用-aws-sdk-操作-S3对象存储/","link":"","permalink":"https://yemilice.com/2019/10/25/Golang-%E8%B0%83%E7%94%A8-aws-sdk-%E6%93%8D%E4%BD%9C-S3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/","excerpt":"","text":"Golang 调用 aws-sdk 操作 S3对象存储前言因为业务问题，要写一个S3对象存储管理代码，由于一直写Go，所以这次采用了Go，Go嘛，快，自带多线程，这种好处就不用多说了吧。 基础的功能 查看S3中包含的bucket bucket中的文件/文件夹 bucket的删除 bucket的创建 bucket的文件上传 bucket的文件下载 bucket的文件删除 aws-sdk 的安装玩Golang你还能不会那啥？对吧，那啥？那飞机！那飞机场，安上~ 1go get github.com/aws/aws-sdk-go aws-sdk-go 的基础使用构建基础的S3连接访问S3的时候，咱们需要access_key，secret_key，对象存储访问IP这三个参数，我们首先要创建一个aws的config，说白了，我们需要定义aws的配置，这样它才知道要怎么访问，去哪里访问等问题。构建一个S3连接代码如下 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"os\" \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/credentials\" _ \"github.com/aws/aws-sdk-go/service/s3/s3manager\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/service/s3\")func main() &#123; access_key := \"xxxxxxxxxxxxx\" secret_key := \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" end_point := \"http://xx.xx.xx.xx:7480\" //endpoint设置，不要动 sess, err := session.NewSession(&amp;aws.Config&#123; Credentials: credentials.NewStaticCredentials(access_key, secret_key, \"\"), Endpoint: aws.String(end_point), Region: aws.String(\"us-east-1\"), DisableSSL: aws.Bool(true), S3ForcePathStyle: aws.Bool(false), //virtual-host style方式，不要修改 &#125;)&#125; 这时候需要你自己去定义一下access_key，secret_key，end_point这三个参数接下来所有的代码，都是以这个连接模板，为核心，后面我就用同上代替配置，请注意！所有的代码都传到GIT上了，到时候会给出地址，不懂得copy下来吧！ 查看S3中包含的bucket查看所有的bucket 1234567891011121314151617181920212223242526272829303132package mainimport ( 导入包同上)func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 svc := s3.New(sess) result, err := svc.ListBuckets(nil) if err != nil &#123; exitErrorf(\"Unable to list buckets, %v\", err) &#125; fmt.Println(\"Buckets:\") for _, b := range result.Buckets &#123; fmt.Printf(\"* %s created on %s\\n\", aws.StringValue(b.Name), aws.TimeValue(b.CreationDate)) &#125; for _, b := range result.Buckets &#123; fmt.Printf(\"%s\\n\", aws.StringValue(b.Name)) &#125; &#125; 列出bucket中的文件/文件夹查看某个bucket中包含的文件/文件夹 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/aws/credentials\" \"github.com/aws/aws-sdk-go/service/s3\" \"fmt\" \"os\")func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 // bucket后跟，go run ....go bucketname bucket := os.Args[1] fmt.Printf(bucket) fmt.Printf(\"\\n\") svc := s3.New(sess) params := &amp;s3.ListObjectsInput&#123; Bucket: aws.String(bucket), &#125; resp, err := svc.ListObjects(params) if err != nil &#123; exitErrorf(\"Unable to list items in bucket %q, %v\", bucket, err) &#125; for _, item := range resp.Contents &#123; fmt.Println(\"Name: \", *item.Key) fmt.Println(\"Last modified:\", *item.LastModified) fmt.Println(\"Size: \", *item.Size) fmt.Println(\"Storage class:\", *item.StorageClass) fmt.Println(\"\") &#125; &#125; bucket的创建创建bucket 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( 导包同上)func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 bucket := os.Args[1] if len(os.Args) != 2 &#123; exitErrorf(\"Bucket name required\\nUsage: %s bucket_name\", os.Args[0]) &#125; // Create S3 service client svc := s3.New(sess) params := &amp;s3.CreateBucketInput&#123; Bucket: aws.String(bucket), &#125; _, err = svc.CreateBucket(params) if err != nil &#123; exitErrorf(\"Unable to create bucket %q, %v\", bucket, err) &#125; // Wait until bucket is created before finishing fmt.Printf(\"Waiting for bucket %q to be created...\\n\", bucket) err = svc.WaitUntilBucketExists(&amp;s3.HeadBucketInput&#123; Bucket: aws.String(bucket), &#125;) if err != nil &#123; exitErrorf(\"Error occurred while waiting for bucket to be created, %v\", bucket) &#125; fmt.Printf(\"Bucket %q successfully created\\n\", bucket)&#125; bucket的文件上传往某个固定的bucket里传文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/aws/credentials\" \"github.com/aws/aws-sdk-go/service/s3/s3manager\" \"fmt\" \"os\")func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 if len(os.Args) != 3 &#123; exitErrorf(\"bucket and file name required\\nUsage: %s bucket_name filename\", os.Args[0]) &#125; bucket := os.Args[1] filename := os.Args[2] file, err := os.Open(filename) if err != nil &#123; exitErrorf(\"Unable to open file %q, %v\", err) &#125; defer file.Close() uploader := s3manager.NewUploader(sess) _, err = uploader.Upload(&amp;s3manager.UploadInput&#123; Bucket: aws.String(bucket), Key: aws.String(filename), Body: file, &#125;) if err != nil &#123; // Print the error and exit. exitErrorf(\"Unable to upload %q to %q, %v\", filename, bucket, err) &#125; fmt.Printf(\"Successfully uploaded %q to %q\\n\", filename, bucket)&#125; bucket的文件下载下载某个bucket中的某个文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package mainimport ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/aws/credentials\" \"github.com/aws/aws-sdk-go/service/s3\" \"github.com/aws/aws-sdk-go/service/s3/s3manager\" \"fmt\" \"os\")func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 if len(os.Args) != 3 &#123; exitErrorf(\"Bucket and item names required\\nUsage: %s bucket_name item_name\", os.Args[0]) &#125; bucket := os.Args[1] item := os.Args[2] file, err := os.Create(item) if err != nil &#123; exitErrorf(\"Unable to open file %q, %v\", err) &#125; defer file.Close() downloader := s3manager.NewDownloader(sess) numBytes, err := downloader.Download(file, &amp;s3.GetObjectInput&#123; Bucket: aws.String(bucket), Key: aws.String(item), &#125;)if err != nil &#123; exitErrorf(\"Unable to download item %q, %v\", item, err)&#125;fmt.Println(\"Downloaded\", file.Name(), numBytes, \"bytes\")&#125; bucket的文件删除删除某个bucket里面的某个文件 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/aws/credentials\" \"github.com/aws/aws-sdk-go/service/s3\" \"fmt\" \"os\")func exitErrorf(msg string, args ...interface&#123;&#125;) &#123; fmt.Fprintf(os.Stderr, msg+\"\\n\", args...) os.Exit(1)&#125;func main() &#123; 配置同上 if len(os.Args) != 3 &#123; exitErrorf(\"Bucket and object name required\\nUsage: %s bucket_name object_name\", os.Args[0]) &#125; bucket := os.Args[1] obj := os.Args[2] svc := s3.New(sess) _, err = svc.DeleteObject(&amp;s3.DeleteObjectInput&#123;Bucket: aws.String(bucket), Key: aws.String(obj)&#125;) if err != nil &#123; exitErrorf(\"Unable to delete object %q from bucket %q, %v\", obj, bucket, err) &#125; err = svc.WaitUntilObjectNotExists(&amp;s3.HeadObjectInput&#123; Bucket: aws.String(bucket), Key: aws.String(obj), &#125;) fmt.Printf(\"Object %q successfully deleted\\n\", obj)&#125; 代码所在地https://github.com/Alexanderklau/Go_poject/tree/master/Go-Storage","categories":[],"tags":[{"name":"前后端","slug":"前后端","permalink":"https://yemilice.com/tags/%E5%89%8D%E5%90%8E%E7%AB%AF/"}],"keywords":[]},{"title":"程序员如何锻炼自己的产品思维","slug":"程序员如何锻炼自己的产品思维","date":"2019-10-25T01:46:55.000Z","updated":"2019-10-25T02:33:21.668Z","comments":true,"path":"2019/10/25/程序员如何锻炼自己的产品思维/","link":"","permalink":"https://yemilice.com/2019/10/25/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E9%94%BB%E7%82%BC%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4/","excerpt":"","text":"程序员如何锻炼自己的产品思维写作目的源于一次需求会议被怼，大老板总是说我以技术思维决定一切。后来我一思考，卧槽，果然是这样，每次我都是非常纠结于技术实现和技术细节，总是纠缠在业务实现里面，所以渐渐就养成了那个习惯。思考了一下，有些地方也的确是应该做出一点点改变了，老话说，种一棵树，最早是十年前，其次就是现在，那么我们就开始种树吧。趁着放假读几本产品思维的书，有一点点感悟，所以将此文写下，方便自己，也方便急于转型的各位程序员老哥。 个人技术背景1.菜逼一个，看博客就知道了。 2.掌握技术：后端： Python，Golang，Java移动端：OC（大学时候兼职IOS开发）前端：基础的React和Vue框架 3.算法技术：渣渣 4.没有任职过任何产品岗位 个人分析优点技术涉及面广，做项目多，一直在项目第一线，熟悉项目业务，思维活跃，善于解决和发现问题。 缺点算法能力差，前端能力差，抽象思维较弱，容易钻牛角尖，对项目整体了解不够透彻，只了解某些模块和部分。 什么是产品思维？理论上的产品思维1.把握关键点的能力 2.出方案，协调资源，说服团队把资源倾斜到关键点上的能力 3.评估关键点进展程度的能力 大白话解释1.首先用户就是一切，一切为了用户爽 2.反向思维，逆推解决问题 3.换位思考啊大哥，你不仅仅是产品人员，你还得是老板，用户，程序员balabala 4.脑子里对业务and产品都算是有了解（当然不能说像程序员一样） 其他来源对产品思维的解释1.从人性本质挖掘需求 说白了就是你要从人去思考问题，也就是说，你要人的表面挖掘到人的内心，类似挠痒痒，不能挠痒痒之后把皮整破了，这就是你满足了表面需求，但是破坏了底层需求2.从赚钱的角度思考 说白了就是追逐利益，想法儿怎么搞钱，例如扫码送东西，例如扫码给红旗，例如十一的时候给微信加国旗，这都是从逐利的思想去发觉需求3.沟通能力 我缺乏哪些技能？粗略看一下，其实缺乏的东西看起来很简单 对整体架构了解不多 逆向思维较差，不能从用户需求去理解问题，只单纯纠结能不能实现功能 评估项目和关键点能力不足 不能够和其他程序员有很好的沟通 平常和业务纠缠太多了，我这种Code monkey每天都去思考这个功能怎么实现，用什么技术更牛逼，怎么优化之类的，纠结技术，功能，细节等等。举个例子，我作为一个代码工程师工程师思维关注技术至上，技术水平代表实力，向于在产品中使用先进、流行的技术，因为掌握先进主流的技术可以提高他的身价。产品思维关注的是，这技术能给用户带来什么价值？有什么商业价值？所以我需要跳出这个怪圈，学会用产品的思维去思考问题，这样也能够开拓自己的眼界，无论是技术还是其他的路，都可以走的更远。 我该如何去补强这些技能？我理解的产品思维每一个项目都是产品。我们可以把工作当中的任何一个输出成果当做产品，用产品思维来完成这个成果。比如，我现在正在开发一个分布式的同步备份工程，将之称为产品。按照产品思维来策划这个工程，你要思考：我为什么要做这个产品？希望得到什么？用户是谁？谁在用这个？他们希望怎么去用？干系人有哪些？他们的期待是？使用场景：现有的web？还是独立开发APP？或者是普通的云计算服务？或者是普通存储服务？或者是类似同步服务？用户的关注点：怎么用？好操控么？用着舒服么？界面看着开心么？思考一下，产品思维的确和工程师思维不太一样，我也不能总是在工程师思维这个怪圈中徘徊 理论上的补强手段保持自己对于不同产品、不同领域的好奇心和敏感度很多时候我都忙于自己当下的工作，很难有机会接触到不同领域不同产品。很可能渐渐地就失去了对于产品的好奇心和敏感度，所以必须要让自己走出去，多去接触，或者看一看别人的产品or项目如何设计，思考他们是怎么做产品的？他们为什么这么做 ？如果我来做能怎样做？通过这样的思考和练习，来保持自己对产品的好奇心和敏感度 向上拓展自己的能力，不能停留于技术人员or产品经理一直纠缠与技术实现细节，总归是只有一层，如果满足这一层，也就是写代码的工具，或者是模块添加人员，也就没有办法建立起来自己的核心竞争力，笑傲江湖里面，剑宗气宗之争也是这一点，剑气双休才是最重要的。所以不仅仅是技术要抓，思想也要抓。 强化自己逻辑思维分析能力在逻辑思维方面，我想没有谁比得过程序员，程序员本来就是逻辑性很强的工作，这一点其实我认为更重要的是换位分析，易地而处的一种状态。我们往往分析自己的工作比较容易，但是涉及大局分析，就有些力不从心，这个我认为还是要针对性训练。 分解问题的能力其实这个在写代码的时候也经常预见到，不是么，一个大问题细化为好几个小问题，换算成产品思维也即是：1.产品有哪些功能？ 2.这些功能下面又分哪些模块？ 3.具体的应用场景在哪里？ 4.产品模块之间相互的联系是什么？ 5.谁在用这些产品？ 6.业务部门之间的需求是否互相耦合？是否已经存在重复需求？ 这里只是举个例子，具体问题具体分析，将自己想象成产品经理，先不要思考问题怎么解决，看看产品是怎么做的，再去对比思路思考解决问题。 用户行为分析能力什么是用户行为？关键就是用户用着你这个产品产生的行为，再去产生其他行为，这是用户增长和用产品化的重要组成部分。首先我们需要找出，关键用户行为，也就是，用户在使用产品时，是奔着你产品的什么方面来的，拿我正在做的项目举个例子，同步备份模块，干嘛的，同步备份文件的，特点呢？分布式，速度快，那不就完了！关键行为就是同步备份，这才是用户的关键行为。首先我们要考虑为什么会产生关键行为，也就输确定产品的价值，产品的价值就是用户愿不愿意给这个产品花钱，愿不愿意花钱去买我们的东西，解决痛点是第一位，但是在这之前，有没有类似的产品做了？人家做的好不好？谁的效率高？谁更牛逼？牛逼在哪？这才是要去分析的地方。 场景分析能力说白了，角色扮演，你把自己想象成一个用户，现在我想要一个产品，思考一下1.产品包含哪些场景 2.产品涉及哪些角色 3.场景会被第三方影响么？如果会，该如何去降低它？ 举个例子，同步备份的产品用在普通用户手中，普通用户的网很慢，同步时断时续，这就是第三方的缘故，但是我们的产品是单节点，也就是说只能一点点下，不能分布式，这就坑了，用户会觉得，你这怎么那么卡，你看看人迅雷，都能断点，都能分布式，你这个，get out，这就是第三方影响使用场景。再举个例子哈：朱啸虎先生在杭州的一次演讲中提到了维诺城。维诺城是在地铁口放置终端，用户出了地铁口之后可以在上面打印周围商家的优惠券。维诺城最初的生意非常好，因为地铁人流大，又是优惠券提供，在大众点评美团还没崛起的时候，它确实是很方便的产品。然而现在一方面因为美团这些APP的强势崛起，另一方面因为地铁提高了租金，更多的商家进场，甚至地铁公司本身都要来抢这个生意，维诺城的生意就下坡路了。 维诺城的例子说明什么？说明如果产品的主要场景容易受限于特殊的场地和时间特性，而这个场景进入的门槛比较低或是由第三方来控制，那么这个产品从场景上来说是有很高风险的；作为产品经理就要尝试思考有没有办法去降低这些影响，或是去发现自己产品不过度依赖这个场景的核心竞争力？ 数据分析能力数据这玩意，永远是支撑一个产品，或者是一个理论的重要依据。如何在通过数据去引导自己的产品思维 1、明确数据指标的定义、口径和使用场景。 要能清楚地和开发人员描述数据指标到底是什么，有哪些维度，在哪个页面或哪个场景之下发生； 2、层层剥离，穷举指标 产品经理为了保证数据的准确性，要尽可能地将指标拆解，拆解到不能拆解为止。同时也要分清哪些是核心指标，哪些是主要指标，哪些是次要指标； 3、数据指标和用户结合 新用户做了什么？老用户做了什么？付费用户做了什么？非付费用户又做了什么？流失用户在流失之前做了什么？要回答这些问题就要将数据指标和不同的用户结合起来分析","categories":[],"tags":[{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]},{"title":"新旧博客迁移的一点感悟","slug":"新旧博客迁移的一点感悟","date":"2019-10-24T09:50:37.000Z","updated":"2020-05-19T07:43:24.550Z","comments":true,"path":"2019/10/24/新旧博客迁移的一点感悟/","link":"","permalink":"https://yemilice.com/2019/10/24/%E6%96%B0%E6%97%A7%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E7%9A%84%E4%B8%80%E7%82%B9%E6%84%9F%E6%82%9F/","excerpt":"","text":"算是日记吧其实自己一直在cnblog上更新自己的博客，从16年3月入行到现在，已经过去了三个春秋，发觉自己的技术还是个渣渣，最近生活真的忙成一团，工作也很忙，生活也很忙，有时候饭都不容易吃上，感觉自己学习真的学不动了，不知道自己是不是懈怠了，运动，音乐什么的，也停下来了，感觉真的缺少了一点点乐趣。不过说真的，自己也真的该动起来了，因为时间不等人啊，已经入行这么久，跌跌撞撞像个摇摆人，所以未来我还是要多学习，多更新我的博客，将自己的技术或者是一些感悟，哪怕是一些灵光一闪的理想，都记录下来，哈哈，这样会不会好一些呢？会不会明天就是更好的那一天呢？ 算是自己我介绍吧渣渣一个，技术又全又杂，精通的少，各种都会一点，但是真正就是个渣渣，哈哈，现在孑然一生，18年年底被踹，重新出发，想想自己当时为了感情选择留在这个陌生的城市，现在已经爱上了这座城市，这座美丽的西南大都会。我想我会留下来的吧，我会留下来的？我也不确定吧，现在要慢慢出发，重新出发，我想我会越来越好的。 算是结尾吧可能你因为一些别的原因走到我博客来，其实欢迎你，欢迎你听一个24岁的家伙碎碎念那么久，未来大部分应该会更新技术，或者是我的一些奇特爱好，哈哈，希望你们会喜欢，我爱你们。","categories":[],"tags":[{"name":"其他技术","slug":"其他技术","permalink":"https://yemilice.com/tags/%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF/"}],"keywords":[]}]}